{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc47e32",
   "metadata": {},
   "source": [
    "# uniprot.core Pydantic Model Generation\n",
    "\n",
    "This notebook generates Pydantic models from LinkML schema files exported from the main schema extraction workflow.\n",
    "\n",
    "**Prerequisites:** Run `uniprot.core.ipynb` first to generate the LinkML YAML schema file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615842bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and setup\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Dataset parameters\n",
    "dataset_name = uniprot.core\n",
    "\n",
    "# Setup paths\n",
    "working_path = Path().absolute()\n",
    "# use os.path to build paths\n",
    "exports_path = Path(os.path.join(str(working_path), \"..\", \"..\", \"data\", dataset_name))\n",
    "linkml_file = Path(os.path.join(str(exports_path), f\"{dataset_name}_linkml_schema.yaml\"))\n",
    "\n",
    "# These will be logged after logger is set up in next cell\n",
    "print(f\"Working directory: {working_path}\")\n",
    "print(f\"Exports path: {exports_path}\")\n",
    "print(f\"LinkML schema file: {linkml_file}\")\n",
    "print(f\"LinkML file exists: {linkml_file.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21038d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from linkml.generators.erdiagramgen import ERDiagramGenerator\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from linkml.generators.pydanticgen import PydanticGenerator\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Setup notebook logger for pydantic generation\n",
    "logger = logging.getLogger(dataset_name or \"pydantic_notebook\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Also configure the rdfsolve.parser logger for any parser operations\n",
    "parser_logger = logging.getLogger('rdfsolve.parser')\n",
    "parser_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Avoid adding duplicate handlers if the cell is re-run\n",
    "if not logger.handlers:\n",
    "    fmt = logging.Formatter(\"%(asctime)s %(levelname)s %(name)s: %(message)s\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    sh = logging.StreamHandler(sys.stdout)\n",
    "    sh.setLevel(logging.DEBUG)  # Use DEBUG to see parser query details\n",
    "    sh.setFormatter(fmt)\n",
    "    logger.addHandler(sh)\n",
    "    \n",
    "    # Add the same handler to the parser logger\n",
    "    parser_logger.addHandler(sh)\n",
    "\n",
    "logger.info(\"Pydantic notebook logger initialized. All logs will be displayed in notebook.\")\n",
    "parser_logger.info(\"Parser logger configured for query tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify LinkML schema file exists\n",
    "if not linkml_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"LinkML schema file not found: {linkml_file}\\n\"\n",
    "        f\"Please run {dataset_name}_schema.ipynb first to generate the schema.\"\n",
    "    )\n",
    "\n",
    "logger.info(\"Found LinkML schema: %s\", linkml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e617875",
   "metadata": {},
   "source": [
    "## Load and Inspect LinkML Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72753984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LinkML schema\n",
    "sv = SchemaView(str(linkml_file))\n",
    "linkml_schema = sv.schema\n",
    "\n",
    "display(Markdown(f\"**Parsed LinkML schema:** Classes = {len(sv.all_classes())}, Slots = {len(sv.all_slots())}\"))\n",
    "\n",
    "# Show basic schema info\n",
    "logger.info(\"Schema ID: %s\", linkml_schema.id)\n",
    "logger.info(\"Schema name: %s\", linkml_schema.name)\n",
    "logger.info(\"Description: %s\", linkml_schema.description)\n",
    "logger.info(\"Classes (%d):\", len(sv.all_classes()))\n",
    "for class_name in sorted(sv.all_classes())[:10]:  # Show first 10\n",
    "    logger.info(\"  - %s\", class_name)\n",
    "if len(sv.all_classes()) > 10:\n",
    "    logger.info(\"  ... and %d more\", len(sv.all_classes()) - 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dda6fd",
   "metadata": {},
   "source": [
    "## Generate Mermaid Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display Mermaid class diagram\n",
    "mermaid_code = ERDiagramGenerator(str(linkml_file)).serialize()\n",
    "\n",
    "display(Markdown(mermaid_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f34c9",
   "metadata": {},
   "source": [
    "## Generate Pydantic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce790bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Pydantic models from LinkML schema\n",
    "src = PydanticGenerator(str(linkml_file)).serialize()\n",
    "\n",
    "logger.info(\"Generated Pydantic code: %d characters\", len(src))\n",
    "logger.info(\"Preview (first 500 chars):\\n%s...\", src[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the generated Pydantic code and extract models\n",
    "ns = {}\n",
    "exec(src, ns)\n",
    "\n",
    "# Find the Pydantic model classes\n",
    "def _is_pydantic_model(name, val):\n",
    "    \"\"\"Check if this is likely a generated Pydantic model class\"\"\"\n",
    "    # Must be a class (type) and have at least one model field\n",
    "    if not isinstance(val, type):\n",
    "        return False\n",
    "    try:\n",
    "        has_model_fields = 0 < len(getattr(val, \"model_fields\", {}))\n",
    "    except:\n",
    "        has_model_fields = False\n",
    "\n",
    "    return has_model_fields\n",
    "\n",
    "pydantic_models = {k: v for k, v in ns.items() if _is_pydantic_model(k, v)}\n",
    "\n",
    "display(Markdown(f\"**Found {len(pydantic_models)} Pydantic model classes for schema.**\"))\n",
    "\n",
    "# Make models available in globals for interactive use\n",
    "for name, cls in pydantic_models.items():\n",
    "    globals()[name] = cls\n",
    "    \n",
    "logger.info(\"Available Pydantic models:\")\n",
    "for name in sorted(pydantic_models.keys())[:10]:  # Show first 10\n",
    "    logger.info(\"  - %s\", name)\n",
    "if len(pydantic_models) > 10:\n",
    "    logger.info(\"  ... and %d more\", len(pydantic_models) - 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a1bf2",
   "metadata": {},
   "source": [
    "## Inspect Generated Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd51622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed information about generated Pydantic classes\n",
    "def show_fields(cls):\n",
    "    if hasattr(cls, 'model_fields'):\n",
    "        fields = list(cls.model_fields.items())\n",
    "        field_list = []\n",
    "        for name, info in fields:\n",
    "            field_list.append(f\"  - `{name}`: {info.annotation}\")\n",
    "        return \"\\n\".join(field_list)\n",
    "    return \"  No fields found\"\n",
    "\n",
    "# Show all available classes (limit output for readability)\n",
    "if pydantic_models:\n",
    "    markdown_output = f\"**Generated Pydantic classes ({len(pydantic_models)} total):**\\n\\n\"\n",
    "    \n",
    "    # Show first 5 classes in detail\n",
    "    shown_classes = list(sorted(pydantic_models.keys()))[:5]\n",
    "    for name in shown_classes:\n",
    "        markdown_output += f\"### {name}\\n\"\n",
    "        markdown_output += show_fields(pydantic_models[name]) + \"\\n\\n\"\n",
    "        \n",
    "    if len(pydantic_models) > 5:\n",
    "        remaining = list(sorted(pydantic_models.keys()))[5:]\n",
    "        markdown_output += f\"### Additional Classes ({len(remaining)})\\n\"\n",
    "        for name in remaining[:20]:  # Show up to 20 more names\n",
    "            markdown_output += f\"- {name}\\n\"\n",
    "        if len(remaining) > 20:\n",
    "            markdown_output += f\"- ... and {len(remaining) - 20} more\\n\"\n",
    "            \n",
    "    display(Markdown(markdown_output))\n",
    "else:\n",
    "    display(Markdown(\"**No pydantic_models found**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a930eb25",
   "metadata": {},
   "source": [
    "## Export Generated Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b16ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated Pydantic code to a Python file\n",
    "pydantic_output_file = Path(os.path.join(str(exports_path), f\"{dataset_name}_pydantic_models.py\"))\n",
    "\n",
    "with open(pydantic_output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(f'\"\"\"\\nPydantic models generated from LinkML schema for {dataset_name}\\n\\nGenerated from: {linkml_file.name}\\n\"\"\"\\n\\n')\n",
    "    f.write(src)\n",
    "\n",
    "logger.info(\"Saved Pydantic models to: %s\", pydantic_output_file)\n",
    "logger.info(\"File size: %.1f KB\", pydantic_output_file.stat().st_size / 1024)\n",
    "\n",
    "# Also show the generated code in the notebook (truncated)\n",
    "display(Markdown(f\"### Generated Pydantic Code\\n\\n```python\\n{src[:2000]}\\n# ... (truncated, see full code in {pydantic_output_file.name})\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d0074",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef92d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the generated Pydantic models\n",
    "if pydantic_models:\n",
    "    # Get the first model class for demonstration\n",
    "    example_class_name = list(pydantic_models.keys())[0]\n",
    "    example_class = pydantic_models[example_class_name]\n",
    "    \n",
    "    logger.info(\"Example usage of %s:\", example_class_name)\n",
    "    logger.info(\"\\n# Import the model\")\n",
    "    logger.info(\"from %s_pydantic_models import %s\", dataset_name, example_class_name)\n",
    "    \n",
    "    logger.info(\"\\n# Create an instance\")\n",
    "    logger.info(\"# %s(...)\", example_class_name)\n",
    "    \n",
    "    # Show model fields\n",
    "    if hasattr(example_class, 'model_fields'):\n",
    "        fields = list(example_class.model_fields.keys())[:5]  # First 5 fields\n",
    "        logger.info(\"\\n# Available fields: %s\", ', '.join(fields))\n",
    "        if len(example_class.model_fields) > 5:\n",
    "            logger.info(\"# ... and %d more\", len(example_class.model_fields) - 5)\n",
    "            \n",
    "    logger.info(\"\\n# Get model schema\")\n",
    "    logger.info(\"schema = %s.model_json_schema()\", example_class_name)\n",
    "else:\n",
    "    logger.info(\"No Pydantic models available for examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30787595",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully generated Pydantic models from the LinkML schema. The models are now available for:\n",
    "\n",
    "- **Data validation**: Use the models to validate data structures\n",
    "- **API development**: Use as request/response models in FastAPI or similar\n",
    "- **Data serialization**: Convert between Python objects and JSON/dict formats\n",
    "- **IDE support**: Get type hints and autocompletion in your IDE\n",
    "\n",
    "### Output Files\n",
    "- **Pydantic models**: `{dataset_name}_pydantic_models.py`\n",
    "- **Source LinkML schema**: `{dataset_name}_linkml_schema.yaml`\n",
    "\n",
    "### Next Steps\n",
    "1. Import the generated models in your Python projects\n",
    "2. Use the models for data validation and serialization\n",
    "3. Integrate with web frameworks like FastAPI for API development"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
