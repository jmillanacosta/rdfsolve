{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d16b10",
   "metadata": {},
   "source": [
    "# mesh.heading Schema Extraction\n",
    "\n",
    "This notebook demonstrates RDF schema extraction from the mesh.heading SPARQL endpoint by discovering or querying for VoID (Vocabulary of Interlinked Datasets) descriptions and some downstream uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ded37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Configuration\n",
    "import os\n",
    "\n",
    "# Dataset parameters\n",
    "endpoint_url = \"https://idsm.elixir-czech.cz/sparql/endpoint/idsm\"\n",
    "dataset_name = \"mesh.heading\"\n",
    "void_iri = \"http://id.nlm.nih.gov/mesh/heading\"\n",
    "graph_uri = \"http://id.nlm.nih.gov/mesh/heading\"\n",
    "\n",
    "# Setup paths\n",
    "working_path = os.path.abspath(\"\")\n",
    "exports_path = os.path.join(working_path, \"..\", \"..\", \"docs\", \"notebooks\", dataset_name)\n",
    "os.makedirs(exports_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cfa4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "from rdfsolve.void_parser import VoidParser\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from linkml.generators.pydanticgen import PydanticGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce8103",
   "metadata": {},
   "source": [
    "## Discover or get VoID Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = VoidParser.from_endpoint_with_discovery(\n",
    "    endpoint_url=endpoint_url,\n",
    "    dataset_name=dataset_name,\n",
    "    exports_path=exports_path,\n",
    "    #exclude_graph_patterns=[\"openlinksw\", \"well-known\"], # Filter out administrative graphs, service descriptions, etc\n",
    "    counts=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "discovery_df = vp.to_schema(\n",
    "    filter_void_admin_nodes=True\n",
    ")  # to filter out unwanted graphs here (TODO improve logic, add step when querying)\n",
    "discovery_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dddbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "discovery_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f5f39",
   "metadata": {},
   "source": [
    "## Class Partition Coverage Analysis\n",
    "Query again to know how many times do we find instances of each \"shape\" in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bdab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(exports_path, f\"{dataset_name}_coverage.csv\")\n",
    "        \n",
    "instance_counts, class_mappings, coverage_stats = vp.analyze_class_partition_usage(\n",
    "            endpoint_url=endpoint_url,\n",
    "            sample_limit=None\n",
    ")\n",
    "        \n",
    "coverage_df = vp.export_coverage_analysis(\n",
    "    coverage_stats, output_file=output_path\n",
    ")\n",
    "\n",
    "print(f\"Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a530dd",
   "metadata": {},
   "source": [
    "## Schema Pattern Coverage Analysis\n",
    "For each subject class type, calculate how many entities participate in each schema pattern divided by the total number of entities of that class type. This gives coverage ratios showing what percentage of entities actually use each relationship pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b864b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate schema pattern coverage ratios\n",
    "frequencies_df = vp.count_schema_shape_frequencies(\n",
    "    endpoint_url=endpoint_url,\n",
    ")\n",
    "\n",
    "# Show top patterns by coverage\n",
    "frequencies_df[['subject_class', 'property', 'object_class', 'coverage_percent']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d82e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export coverage analysis\n",
    "frequencies_output_path = os.path.join(exports_path, f\"{dataset_name}_pattern_coverage.csv\")\n",
    "exported_df = vp.export_schema_shape_frequencies(frequencies_df, output_file=frequencies_output_path)\n",
    "\n",
    "# Simple summary\n",
    "if not frequencies_df.empty:\n",
    "    avg_coverage = frequencies_df['coverage_percent'].mean()\n",
    "    high_coverage = (frequencies_df['coverage_percent'] > 50).sum()\n",
    "    print(f\"Average pattern coverage: {avg_coverage:.1f}%\")\n",
    "    print(f\"Patterns with >50% coverage: {high_coverage}/{len(frequencies_df)}\")\n",
    "    print(f\"Exported to: {frequencies_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db811da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pattern coverage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not frequencies_df.empty:\n",
    "    # Simple bar chart of top 15 patterns by coverage\n",
    "    top_patterns = frequencies_df\n",
    "\n",
    "    plt.figure(figsize=(40, 60 * len(frequencies_df) / 100))\n",
    "    bars = plt.barh(range(len(top_patterns)), top_patterns[\"coverage_percent\"])\n",
    "    plt.yticks(\n",
    "        range(len(top_patterns)),\n",
    "        [\n",
    "            f\"{row['subject_class']} {row['property']} {row['object_class']}\"\n",
    "            for _, row in top_patterns.iterrows()\n",
    "        ],\n",
    "    )\n",
    "    plt.xlabel(\"Coverage (%)\")\n",
    "    plt.title(f\"Schema Pattern Coverage in {dataset_name}\")\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # Add percentage labels\n",
    "    for i, (bar, pct) in enumerate(zip(bars, top_patterns[\"coverage_percent\"])):\n",
    "        plt.text(\n",
    "            bar.get_width() + 1,\n",
    "            bar.get_y() + bar.get_height() / 2,\n",
    "            f\"{pct:.1f}%\",\n",
    "            va=\"center\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No coverage data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9920263",
   "metadata": {},
   "source": [
    "## LinkML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2baa043",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = f\"{dataset_name}_schema\"\n",
    "yaml_text = vp.to_linkml_yaml(\n",
    "    schema_name=schema_name,\n",
    "    schema_description=f\"LinkML schema for {dataset_name}\",\n",
    "    filter_void_nodes=True)\n",
    "\n",
    "# Save to LinkML YAML\n",
    "linkml_file = os.path.join(exports_path, f\"{dataset_name}_linkml_schema.yaml\")\n",
    "with open(linkml_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(yaml_text)\n",
    "print('LinkML saved to', linkml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7429b5",
   "metadata": {},
   "source": [
    "### Mermaid  diagram for LinkML Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a4753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = SchemaView(linkml_file)\n",
    "linkml_schema = sv.schema\n",
    "\n",
    "print(\"Parsed LinkML schema: Classes =\", len(sv.all_classes()), \"Slots =\", len(sv.all_slots()))\n",
    "\n",
    "# Build and display a Mermaid class diagram for the mesh.heading LinkedML\n",
    "from linkml.generators.erdiagramgen import ERDiagramGenerator\n",
    "\n",
    "mermaid = ERDiagramGenerator(linkml_file).serialize()\n",
    "from IPython.display import display, Markdown, Latex\n",
    "display(Markdown(mermaid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b68c1c",
   "metadata": {},
   "source": [
    "### LinkML pyDantic Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = PydanticGenerator(linkml_file).serialize()\n",
    "ns = {}\n",
    "exec(src, ns)\n",
    "\n",
    "# Find the Pydantic model classes\n",
    "def _is_pydantic_model(name, val):\n",
    "    \"\"\"Check if this is likely a generated Pydantic model class\"\"\"\n",
    "    # Must be a class (type) and have at least one model field\n",
    "    if not isinstance(val, type):\n",
    "        return False\n",
    "    try:\n",
    "        has_model_fields = 0 < len(getattr(val, \"model_fields\", {}))\n",
    "    except:\n",
    "        has_model_fields = False\n",
    "\n",
    "    return has_model_fields\n",
    "\n",
    "pydantic_models = {k: v for k, v in ns.items() if _is_pydantic_model(k, v)}\n",
    "\n",
    "print(f\"Found {len(pydantic_models)} Pydantic model classes for schema.\")\n",
    "\n",
    "# Save all models to globals\n",
    "for name, cls in pydantic_models.items():\n",
    "    globals()[name] = cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096fae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all generated Pydantic classes and their fields for mesh.heading\n",
    "def show_fields(cls):\n",
    "    if hasattr(cls, 'model_fields'):\n",
    "        fields = list(cls.model_fields.items())\n",
    "        for name, info in fields:\n",
    "            print(f\"  {name}: {info.annotation}\")\n",
    "\n",
    "# Show all available classes\n",
    "if 'pydantic_models' in globals() and pydantic_models:\n",
    "    print(f\"All {len(pydantic_models)} generated Pydantic classes:\\n\")\n",
    "    for name in sorted(pydantic_models.keys()):\n",
    "        print(f\"=== {name} ===\")\n",
    "        show_fields(pydantic_models[name])\n",
    "        print()\n",
    "else:\n",
    "    print(\"No pydantic_models found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153b673",
   "metadata": {},
   "source": [
    "## Export Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = os.path.join(exports_path, f\"{dataset_name}_schema.json\")\n",
    "csv_path = os.path.join(exports_path, f\"{dataset_name}_schema.csv\")\n",
    "\n",
    "discovery_df.to_csv(csv_path, index=False)\n",
    "with open(json_path, 'w', encoding='utf-8') as fh:\n",
    "    json.dump(vp.to_json(filter_void_nodes=True), fh, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdfsolve-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
