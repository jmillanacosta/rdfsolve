{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ded37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Configuration\n",
    "import os\n",
    "\n",
    "# Dataset parameters\n",
    "endpoint_url = \"https://idsm.elixir-czech.cz/sparql/endpoint/idsm\"\n",
    "dataset_name = \"mesh.heading\"\n",
    "void_iri = \"http://id.nlm.nih.gov/mesh/heading\"\n",
    "graph_uri = \"http://id.nlm.nih.gov/mesh/heading\"\n",
    "\n",
    "# Setup paths\n",
    "working_path = os.path.abspath(\"\")\n",
    "exports_path = os.path.join(working_path, \"..\", \"..\", \"docs\", \"notebooks\", dataset_name)\n",
    "os.makedirs(exports_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d16b10",
   "metadata": {},
   "source": [
    "# RDFSolve Schema Extraction\n",
    "\n",
    "This notebook demonstrates RDF schema extraction from SPARQL endpoints using VoID (Vocabulary of Interlinked Datasets) descriptions.\n",
    "\n",
    "**Key Features:**\n",
    "- Automatic VoID discovery from endpoints\n",
    "- Multiple extraction modes (fast, complete, simple)\n",
    "- Schema export to CSV, JSON, JSON-LD, and LinkML formats\n",
    "- Class partition coverage analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cfa4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from rdfsolve.rdfsolve import RDFSolver\n",
    "from rdfsolve.void_parser import VoidParser\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928e8e7d",
   "metadata": {},
   "source": [
    "## 1. Initialize RDFSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RDFSolver\n",
    "solver = RDFSolver(\n",
    "    endpoint=endpoint_url,\n",
    "    path=working_path,\n",
    "    void_iri=void_iri,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "# Minimal confirmation\n",
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce8103",
   "metadata": {},
   "source": [
    "## 2. Load VoID Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two concise approaches: (A) high-level (RDFSolver), (B) manual (VoidParser)\n",
    "\n",
    "# Approach A — high level: generate VoID (fast), extract schema, show sample\n",
    "void_output = os.path.join(exports_path, f\"{dataset_name}_generated_void.ttl\")\n",
    "void_graph = solver.void_generator(graph_uris=graph_uri, output_file=void_output, counts=False)\n",
    "parser = solver.extract_schema()\n",
    "schema_df = parser.to_schema(filter_void_nodes=True)\n",
    "\n",
    "print(\"Schema triples:\", len(schema_df))\n",
    "display(schema_df.head())\n",
    "\n",
    "# Approach B — manual (optional): discover VoID and inspect\n",
    "vp = VoidParser.from_endpoint_with_discovery(endpoint_url=endpoint_url, dataset_name=dataset_name, exports_path=exports_path)\n",
    "manual_df = vp.to_schema(filter_void_nodes=True)\n",
    "print(\"Manual discovery triples:\", len(manual_df))\n",
    "# display(manual_df.head())  # uncomment if you need to inspect manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db9486",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_schema_df = existing_schema_df[\n",
    "    ~existing_schema_df[\"subject_uri\"].str.contains(\"openlinksw\", na=False)\n",
    "]\n",
    "filtered_schema_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e4ae88",
   "metadata": {},
   "source": [
    "## 4. Complete VoID Generation (with counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete VoID with counts (may take longer)\n",
    "complete_output = os.path.join(exports_path, f\"{dataset_name}_complete_void.ttl\")\n",
    "complete_void = solver.void_generator(graph_uris=graph_uri, output_file=complete_output, counts=True)\n",
    "complete_parser = VoidParser(complete_void)\n",
    "complete_schema_df = complete_parser.to_schema(filter_void_nodes=True)\n",
    "\n",
    "print(\"Complete schema triples:\", len(complete_schema_df))\n",
    "display(complete_schema_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed4057",
   "metadata": {},
   "source": [
    "## 5. Simple Extraction (Python post-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3085e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple extraction (alternative): quick VoID generation using from_sparql\n",
    "simple_output = os.path.join(exports_path, f\"{dataset_name}_simple_void.ttl\")\n",
    "simple_parser = VoidParser.from_sparql(endpoint_url=endpoint_url, output_file=simple_output, exclude_other_graphs=True)\n",
    "simple_schema_df = simple_parser.to_schema(filter_void_nodes=True)\n",
    "\n",
    "print(\"Simple schema triples:\", len(simple_schema_df))\n",
    "display(simple_schema_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f5f39",
   "metadata": {},
   "source": [
    "## 6. Class Partition Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bdab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class partition usage and coverage\n",
    "analysis_parser = None\n",
    "if 'existing_parser' in globals() and existing_parser is not None:\n",
    "    analysis_parser = existing_parser\n",
    "elif 'simple_parser' in globals() and simple_parser is not None:\n",
    "    analysis_parser = simple_parser\n",
    "elif 'fast_parser' in globals() and fast_parser is not None:\n",
    "    analysis_parser = fast_parser\n",
    "elif 'complete_parser' in globals() and complete_parser is not None:\n",
    "    analysis_parser = complete_parser\n",
    "\n",
    "if analysis_parser:\n",
    "    try:\n",
    "        output_path = os.path.join(exports_path, f\"{dataset_name}_coverage.csv\")\n",
    "        \n",
    "        instance_counts, class_mappings, coverage_stats = analysis_parser.analyze_class_partition_usage(\n",
    "            endpoint_url=endpoint_url,\n",
    "            sample_limit=None\n",
    "        )\n",
    "        \n",
    "        coverage_df = analysis_parser.export_coverage_analysis(\n",
    "            coverage_stats, output_file=output_path\n",
    "        )\n",
    "        \n",
    "        print(f\"Coverage analysis completed\")\n",
    "        print(f\"Instances analyzed: {len(instance_counts):,}\")\n",
    "        print(f\"Class partitions: {len(class_mappings)}\")\n",
    "        print(f\"Saved to: {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Coverage analysis failed: {str(e)}\")\n",
    "        coverage_df = None\n",
    "else:\n",
    "    print(\"No parser available for coverage analysis\")\n",
    "    coverage_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7674c701",
   "metadata": {},
   "source": [
    "## 7. Schema Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f7999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare extraction modes available in this session\n",
    "schemas = {}\n",
    "if 'schema_df' in globals() and schema_df is not None:\n",
    "    schemas['HighLevel'] = schema_df\n",
    "if 'complete_schema_df' in globals() and complete_schema_df is not None:\n",
    "    schemas['Complete'] = complete_schema_df\n",
    "if 'simple_schema_df' in globals() and simple_schema_df is not None:\n",
    "    schemas['Simple'] = simple_schema_df\n",
    "if 'manual_df' in globals() and manual_df is not None:\n",
    "    schemas['Manual'] = manual_df\n",
    "\n",
    "if schemas:\n",
    "    rows = []\n",
    "    for name, df in schemas.items():\n",
    "        rows.append({'Mode': name, 'Triples': len(df), 'Classes': df['subject_class'].nunique(), 'Properties': df['property'].nunique()})\n",
    "    comparison_df = pd.DataFrame(rows)\n",
    "    display(comparison_df)\n",
    "\n",
    "    # Top classes and properties in the primary schema\n",
    "    main = list(schemas.values())[0]\n",
    "    top_classes = main['subject_class'].value_counts().head(5).rename_axis('Class').reset_index(name='Triples')\n",
    "    top_props = main['property'].value_counts().head(5).rename_axis('Property').reset_index(name='Usage')\n",
    "    display(top_classes)\n",
    "    display(top_props)\n",
    "else:\n",
    "    print('No schemas to compare')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9920263",
   "metadata": {},
   "source": [
    "## 8. LinkML Schema Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2baa043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LinkML from available parser (use parser from above)\n",
    "linkml_parser = None\n",
    "for candidate in ('complete_parser','parser','simple_parser','vp'):\n",
    "    if candidate in globals() and globals()[candidate] is not None:\n",
    "        linkml_parser = globals()[candidate]\n",
    "        break\n",
    "\n",
    "if linkml_parser:\n",
    "    schema_name = f\"{dataset_name}_schema\"\n",
    "    yaml_text = linkml_parser.to_linkml_yaml(schema_name=schema_name, schema_description=f\"LinkML schema for {dataset_name}\", filter_void_nodes=True)\n",
    "    linkml_file = os.path.join(exports_path, f\"{dataset_name}_linkml_schema.yaml\")\n",
    "    with open(linkml_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(yaml_text)\n",
    "    print('LinkML saved to', linkml_file)\n",
    "    print('Sample (first 200 chars):')\n",
    "    print(yaml_text[:200])\n",
    "else:\n",
    "    print('No parser available for LinkML generation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153b673",
   "metadata": {},
   "source": [
    "## 9. Export Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export available schemas (CSV/JSON) — concise\n",
    "export_count = 0\n",
    "for name, df in list(schemas.items()):\n",
    "    if df is None or len(df) == 0:\n",
    "        continue\n",
    "    safe = name.lower().replace(' ', '_')\n",
    "    csv_path = os.path.join(exports_path, f\"{dataset_name}_{safe}_schema.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    parser = globals().get(f\"{name.lower()}_parser\") or globals().get('parser')\n",
    "    if parser:\n",
    "        json_path = os.path.join(exports_path, f\"{dataset_name}_{safe}_schema.json\")\n",
    "        with open(json_path, 'w', encoding='utf-8') as fh:\n",
    "            json.dump(parser.to_json(filter_void_nodes=True), fh, indent=2)\n",
    "    export_count += 1\n",
    "\n",
    "print('Exported', export_count, 'schemas to', exports_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8566e6e4",
   "metadata": {},
   "source": [
    "## 10. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results summary — concise DataFrame and status\n",
    "summary_rows = []\n",
    "if 'schemas' in globals() and schemas:\n",
    "    total_triples = sum(len(df) for df in schemas.values())\n",
    "    total_classes = sum(df['subject_class'].nunique() for df in schemas.values())\n",
    "    total_properties = sum(df['property'].nunique() for df in schemas.values())\n",
    "    summary_rows = [\n",
    "        {'Metric': 'Total Schema Triples', 'Count': total_triples},\n",
    "        {'Metric': 'Unique Classes', 'Count': total_classes},\n",
    "        {'Metric': 'Unique Properties', 'Count': total_properties},\n",
    "    ]\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "if not summary_df.empty:\n",
    "    display(summary_df)\n",
    "\n",
    "print('Endpoint:', endpoint_url)\n",
    "print('Dataset:', dataset_name)\n",
    "\n",
    "print('Coverage analysis available:' , 'coverage_df' in globals() and coverage_df is not None)\n",
    "print('LinkML generated:' , os.path.exists(os.path.join(exports_path, f\"{dataset_name}_linkml_schema.yaml\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdfsolve-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
