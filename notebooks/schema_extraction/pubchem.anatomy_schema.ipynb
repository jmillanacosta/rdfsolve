{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d16b10",
   "metadata": {},
   "source": [
    "# pubchem.anatomy Schema Extraction\n",
    "\n",
    "This notebook demonstrates RDF schema extraction from the pubchem.anatomy SPARQL endpoint by discovering or querying for VoID (Vocabulary of Interlinked Datasets) descriptions and some downstream uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ded37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Configuration\n",
    "import os\n",
    "\n",
    "# Dataset parameters\n",
    "endpoint_url = \"https://idsm.elixir-czech.cz/sparql/endpoint/idsm\"\n",
    "dataset_name = \"pubchem.anatomy\"\n",
    "void_iri = \"http://rdf.ncbi.nlm.nih.gov/pubchem/anatomy\"\n",
    "graph_uri = \"http://rdf.ncbi.nlm.nih.gov/pubchem/anatomy\"\n",
    "\n",
    "# Setup paths\n",
    "working_path = os.path.abspath(\"\")\n",
    "exports_path = os.path.join(working_path, \"..\", \"..\", \"docs\", \"notebooks\", dataset_name)\n",
    "os.makedirs(exports_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cfa4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "from rdfsolve.void_parser import VoidParser\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Configure Plotly for HTML output\n",
    "import plotly.io as pio\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# Set renderer to 'notebook' for Jupyter, but ensure HTML export works\n",
    "pio.renderers.default = \"notebook+plotly_mimetype\"\n",
    "\n",
    "# Initialize offline mode for Plotly\n",
    "pyo.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce8103",
   "metadata": {},
   "source": [
    "## Discover or get VoID Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp = VoidParser.from_endpoint_with_discovery(\n",
    "    endpoint_url=endpoint_url,\n",
    "    dataset_name=dataset_name,\n",
    "    exports_path=exports_path,\n",
    "    #exclude_graph_patterns=[\"openlinksw\", \"well-known\"], # Filter out administrative graphs, service descriptions, etc\n",
    "    counts=True,\n",
    "    #graph_uri=graph_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "discovery_df = vp.to_schema(\n",
    "    filter_void_admin_nodes=True\n",
    ")  # to filter out unwanted graphs here (TODO improve logic, add step when querying)\n",
    "discovery_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dddbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "discovery_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f5f39",
   "metadata": {},
   "source": [
    "## Class Partition Coverage Analysis\n",
    "Query again to know how many times do we find instances of each \"shape\" in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bdab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(exports_path, f\"{dataset_name}_coverage.csv\")\n",
    "        \n",
    "instance_counts, class_mappings, coverage_stats = vp.analyze_class_partition_usage(\n",
    "            endpoint_url=endpoint_url,\n",
    "            sample_limit=None\n",
    ")\n",
    "        \n",
    "coverage_df = vp.export_coverage_analysis(\n",
    "    coverage_stats, output_file=output_path\n",
    ")\n",
    "\n",
    "display(Markdown(f\"**Saved to:** `{output_path}`\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a530dd",
   "metadata": {},
   "source": [
    "## Schema Pattern Coverage Analysis\n",
    "For each subject class type, calculate how many entities participate in each schema pattern divided by the total number of entities of that class type. This gives coverage ratios showing what percentage of entities actually use each relationship pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b864b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate schema pattern coverage ratios\n",
    "frequencies_df = vp.count_schema_shape_frequencies(\n",
    "    endpoint_url=endpoint_url,\n",
    ")\n",
    "\n",
    "# Show top patterns by coverage\n",
    "frequencies_df[['subject_class', 'property', 'object_class', 'coverage_percent']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d82e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export coverage analysis\n",
    "frequencies_output_path = os.path.join(exports_path, f\"{dataset_name}_pattern_coverage.csv\")\n",
    "exported_df = vp.export_schema_shape_frequencies(frequencies_df, output_file=frequencies_output_path)\n",
    "\n",
    "# Simple summary\n",
    "if not frequencies_df.empty:\n",
    "    avg_coverage = frequencies_df['coverage_percent'].mean()\n",
    "    high_coverage = (frequencies_df['coverage_percent'] > 50).sum()\n",
    "    display(Markdown(f\"\"\"\n",
    "**Pattern Coverage Summary:**\n",
    "- Average pattern coverage: **{avg_coverage:.1f}%**\n",
    "- Patterns with >50% coverage: **{high_coverage}/{len(frequencies_df)}**\n",
    "- Exported to: `{frequencies_output_path}`\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db811da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "\n",
    "if not frequencies_df.empty:\n",
    "    df = frequencies_df.copy()\n",
    "    df[\"coverage_percent\"] = pd.to_numeric(\n",
    "        df[\"coverage_percent\"], errors=\"coerce\"\n",
    "    ).fillna(0)\n",
    "    df = df.sort_values(\"coverage_percent\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    def make_label(row):\n",
    "        return (\n",
    "            f\"<b>{row['subject_class']}</b> \"\n",
    "            f\"<span style='color:#888;'></span> \"\n",
    "            f\"<i>{row['property']}</i> \"\n",
    "            f\"<span style='color:#888;'></span> \"\n",
    "            f\"<b>{row['object_class']}</b>\"\n",
    "        )\n",
    "\n",
    "    df[\"styled_label\"] = df.apply(make_label, axis=1)\n",
    "\n",
    "    text_positions = [\"outside\" if v < 95 else \"inside\" for v in df[\"coverage_percent\"]]\n",
    "    custom_colorscale = [\n",
    "        [0.0, \"#c3d9c0\"],  # muted green\n",
    "        [0.4, \"#e8e4cf\"],  # soft beige\n",
    "        [0.7, \"#e5cdbd\"],  # muted peach\n",
    "        [1.0, \"#d36e61\"],  # subdued red\n",
    "    ]\n",
    "\n",
    "    # Figure sizing\n",
    "    bar_height = 26\n",
    "    fig_height = min(2000, bar_height * len(df) + 200)\n",
    "\n",
    "    fig = go.Figure(\n",
    "        go.Bar(\n",
    "            x=df[\"coverage_percent\"],\n",
    "            y=df[\"styled_label\"],\n",
    "            orientation=\"h\",\n",
    "            text=[f\"{v:.1f}%\" for v in df[\"coverage_percent\"]],\n",
    "            textposition=text_positions,\n",
    "            marker=dict(\n",
    "                color=df[\"coverage_percent\"],\n",
    "                colorscale=custom_colorscale,\n",
    "                cmin=0,\n",
    "                cmax=100,\n",
    "                line=dict(color=\"white\", width=0.6),\n",
    "            ),\n",
    "            hovertemplate=\"<b>%{y}</b><br>Coverage: %{x:.1f}%<extra></extra>\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            \"text\": f\"Schema Pattern Coverage for {dataset_name}\",\n",
    "            \"x\": 0.5,\n",
    "            \"font\": {\"size\": 18},\n",
    "        },\n",
    "        xaxis=dict(\n",
    "            title=\"Coverage (%)\",\n",
    "            range=[0, 100],  # fixed x-axis range\n",
    "            ticksuffix=\"%\",\n",
    "            showgrid=True,\n",
    "            gridcolor=\"rgba(220,220,220,0.3)\",\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"\",\n",
    "            autorange=\"reversed\",\n",
    "            automargin=True,\n",
    "            fixedrange=False,  # allow vertical zoom/pan\n",
    "        ),\n",
    "        template=\"plotly_white\",\n",
    "        autosize=True,  # allow figure to scale with container\n",
    "        height=fig_height,  # base height (will scale)\n",
    "        margin=dict(t=80, b=50, l=480, r=150),  # extra right margin for text\n",
    "        plot_bgcolor=\"white\",\n",
    "        paper_bgcolor=\"white\",\n",
    "    )\n",
    "\n",
    "    # Disable horizontal zoom/pan\n",
    "    fig.update_xaxes(fixedrange=True)\n",
    "\n",
    "    # Show figure with config for HTML export compatibility\n",
    "    fig.show(config={\n",
    "        \"scrollZoom\": True, \n",
    "        \"responsive\": True,\n",
    "        \"toImageButtonOptions\": {\n",
    "            \"format\": \"png\",\n",
    "            \"filename\": f\"{dataset_name}_schema_coverage\",\n",
    "            \"height\": fig_height,\n",
    "            \"width\": 1200,\n",
    "            \"scale\": 1\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Also display as HTML div for better HTML export compatibility\n",
    "    #from IPython.display import HTML\n",
    "    #html_div = pio.to_html(fig, include_plotlyjs='inline', #div_id=f\"plotly-div-{dataset_name}\")\n",
    "    #display(HTML(html_div))\n",
    "    \n",
    "else:\n",
    "    display(Markdown(\"**No coverage data to visualize**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9920263",
   "metadata": {},
   "source": [
    "## LinkML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fdd877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkml.generators.erdiagramgen import ERDiagramGenerator\n",
    "from IPython.display import display, Javascript\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from linkml.generators.pydanticgen import PydanticGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2baa043",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = f\"{dataset_name}_schema\"\n",
    "yaml_text = vp.to_linkml_yaml(\n",
    "    schema_name=schema_name,\n",
    "    schema_description=f\"LinkML schema for {dataset_name}\",\n",
    "    filter_void_nodes=True)\n",
    "\n",
    "# Save to LinkML YAML\n",
    "linkml_file = os.path.join(exports_path, f\"{dataset_name}_linkml_schema.yaml\")\n",
    "with open(linkml_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(yaml_text)\n",
    "display(Markdown(f\"**LinkML saved to:** `{linkml_file}`\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7429b5",
   "metadata": {},
   "source": [
    "### Mermaid  diagram for LinkML Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a4753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = SchemaView(linkml_file)\n",
    "linkml_schema = sv.schema\n",
    "\n",
    "display(Markdown(f\"**Parsed LinkML schema:** Classes = {len(sv.all_classes())}, Slots = {len(sv.all_slots())}\"))\n",
    "\n",
    "# Build and display a Mermaid class diagram for the pubchem.anatomy LinkedML\n",
    "mermaid_code = ERDiagramGenerator(linkml_file).serialize()\n",
    "\n",
    "display(\n",
    "    Javascript(\n",
    "        f\"\"\"\n",
    "require.config({{paths: {{mermaid: \"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min\"}}}});\n",
    "require([\"mermaid\"], function(mermaid) {{\n",
    "    mermaid.initialize({{ startOnLoad: true }});\n",
    "    var container = document.createElement(\"div\");\n",
    "    container.className = \"mermaid\";\n",
    "    container.textContent = `{mermaid_code}`;\n",
    "    document.body.appendChild(container);\n",
    "}});\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b68c1c",
   "metadata": {},
   "source": [
    "### LinkML pyDantic Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = PydanticGenerator(linkml_file).serialize()\n",
    "ns = {}\n",
    "exec(src, ns)\n",
    "\n",
    "# Find the Pydantic model classes\n",
    "def _is_pydantic_model(name, val):\n",
    "    \"\"\"Check if this is likely a generated Pydantic model class\"\"\"\n",
    "    # Must be a class (type) and have at least one model field\n",
    "    if not isinstance(val, type):\n",
    "        return False\n",
    "    try:\n",
    "        has_model_fields = 0 < len(getattr(val, \"model_fields\", {}))\n",
    "    except:\n",
    "        has_model_fields = False\n",
    "\n",
    "    return has_model_fields\n",
    "\n",
    "pydantic_models = {k: v for k, v in ns.items() if _is_pydantic_model(k, v)}\n",
    "\n",
    "display(Markdown(f\"**Found {len(pydantic_models)} Pydantic model classes for schema.**\"))\n",
    "\n",
    "# Save all models to globals\n",
    "for name, cls in pydantic_models.items():\n",
    "    globals()[name] = cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096fae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all generated Pydantic classes and their fields for pubchem.anatomy\n",
    "def show_fields(cls):\n",
    "    if hasattr(cls, 'model_fields'):\n",
    "        fields = list(cls.model_fields.items())\n",
    "        field_list = []\n",
    "        for name, info in fields:\n",
    "            field_list.append(f\"  - `{name}`: {info.annotation}\")\n",
    "        return \"\\n\".join(field_list)\n",
    "    return \"  No fields found\"\n",
    "\n",
    "# Show all available classes\n",
    "if 'pydantic_models' in globals() and pydantic_models:\n",
    "    markdown_output = f\"**All {len(pydantic_models)} generated Pydantic classes:**\\n\\n\"\n",
    "    for name in sorted(pydantic_models.keys()):\n",
    "        markdown_output += f\"### {name}\\n\"\n",
    "        markdown_output += show_fields(pydantic_models[name]) + \"\\n\\n\"\n",
    "    display(Markdown(markdown_output))\n",
    "else:\n",
    "    display(Markdown(\"**No pydantic_models found**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153b673",
   "metadata": {},
   "source": [
    "## Export Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = os.path.join(exports_path, f\"{dataset_name}_schema.json\")\n",
    "csv_path = os.path.join(exports_path, f\"{dataset_name}_schema.csv\")\n",
    "\n",
    "discovery_df.to_csv(csv_path, index=False)\n",
    "with open(json_path, 'w', encoding='utf-8') as fh:\n",
    "    json.dump(vp.to_json(filter_void_nodes=True), fh, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdfsolve-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
