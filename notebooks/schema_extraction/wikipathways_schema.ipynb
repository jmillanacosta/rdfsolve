{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ded37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipathways Dataset Configuration\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    endpoint_url = \"https://sparql.wikipathways.org/sparql/\"\n",
    "    dataset_name = \"wikipathways\"\n",
    "    void_iri = \"http://rdf.wikipathways.org\"\n",
    "    graph_uri = \"http://rdf.wikipathways.org\"\n",
    "    working_path = os.path.abspath(\"\")\n",
    "    # Create the results directory if it does not exist\n",
    "    if not os.path.exists(os.path.join(working_path, \"..\", \"..\", \"results\", dataset_name)):\n",
    "        os.makedirs(os.path.join(working_path, \"..\", \"..\", \"results\", dataset_name))\n",
    "    exports_path = os.path.join(working_path, \"..\", \"..\", \"results\", dataset_name)\n",
    "    print(\"Dataset configuration completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR in dataset configuration: {e}\")\n",
    "    print(\"Full traceback:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d16b10",
   "metadata": {},
   "source": [
    "# RDFSolve: Complete Workflow Demonstration\n",
    "\n",
    "This notebook demonstrates schema extraction and exports. There are different querying modes for mining the graphs:\n",
    "\n",
    "1. **Traditional VoID Generation** - Full COUNT aggregations\n",
    "2. **Fast Discovery Mode** - No COUNT aggregations\n",
    "3. **Sampled Analysis** - Limited sample size for large datasets\n",
    "4. **Simple Extraction Mode** - Python post-processing\n",
    "\n",
    "The exports include:\n",
    "1. **Class Partition Coverage Analysis** - count partition coverage csv.\n",
    "2. **Export Formats** - rdf-config (TODO), JSON, JSON-LD schemas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cfa4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from rdfsolve.rdfsolve import RDFSolver\n",
    "from rdfsolve.void_parser import VoidParser, generate_void_from_endpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928e8e7d",
   "metadata": {},
   "source": [
    "## 1. Dataset Configuration\n",
    "\n",
    "Configure the wikipathways dataset parameters for all analysis modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Dataset Configuration:\")\n",
    "    print(f\"  Dataset: {dataset_name}\")\n",
    "    print(f\"  Endpoint: {endpoint_url}\")\n",
    "    print(f\"  Graph URI: {graph_uri}\")\n",
    "    print(f\"  VoID IRI: {void_iri}\")\n",
    "\n",
    "    # Initialize RDFSolver\n",
    "    solver = RDFSolver(\n",
    "        endpoint=endpoint_url,\n",
    "        path=working_path,\n",
    "        void_iri=void_iri,\n",
    "        dataset_name=dataset_name\n",
    "    )\n",
    "    print(\"RDFSolver initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing RDFSolver: {e}\")\n",
    "    print(\"Full traceback:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0124951",
   "metadata": {},
   "source": [
    "## 2. Test with LIMIT\n",
    "\n",
    "Test whether the schema can be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db9486",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"=== TEST ENDPOINT ===\")\n",
    "    output_path = os.path.join(exports_path, f\"{dataset_name}_fast_void.ttl\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate fast VoID without count aggregations\n",
    "    fast_void_graph = solver.void_generator(\n",
    "        graph_uri=graph_uri,\n",
    "        output_file=output_path,\n",
    "        counts=False,  # Disable COUNT aggregations for speed\n",
    "        sample_limit=10  # Just test if we can extract schema from endpoint/graph\n",
    "    )\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Fast VoID generation completed\")\n",
    "    print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "    print(f\"VoID triples: {len(fast_void_graph):,}\")\n",
    "    print(f\"Output file:\", output_path)\n",
    "\n",
    "    # Extract schema from fast VoID\n",
    "    fast_parser = VoidParser(fast_void_graph)\n",
    "    fast_schema_df = fast_parser.to_schema(filter_void_nodes=True)\n",
    "\n",
    "    print(f\"\\nFast Schema Analysis:\")\n",
    "    print(f\"  Schema triples: {len(fast_schema_df):,}\")\n",
    "    print(f\"  Unique classes: {fast_schema_df['subject_class'].nunique()}\")\n",
    "    print(f\"  Unique properties: {fast_schema_df['property'].nunique()}\")\n",
    "    print(f\"  Unique objects: {fast_schema_df['object_class'].nunique()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR in fast VoID generation: {e}\")\n",
    "    print(\"Full traceback:\")\n",
    "    traceback.print_exc()\n",
    "    fast_void_graph = None\n",
    "    fast_parser = None\n",
    "    fast_schema_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e4ae88",
   "metadata": {},
   "source": [
    "## 2. SPARQL-based VoID Generation\n",
    "\n",
    "Lets the SPARQL query create the partitions and get coverages. Can (and will often) time out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"=== COMPLETE SPARQL VOID GENERATION (WITH COUNTS) ===\")\n",
    "    output_path = os.path.join(exports_path, f\"{dataset_name}_complete_void.ttl\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate complete VoID with COUNT aggregations\n",
    "    complete_void_graph = solver.void_generator(\n",
    "        graph_uri=graph_uri,\n",
    "        output_file=output_path,\n",
    "        counts=True,  # Enable COUNT aggregations\n",
    "    )\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Complete VoID generation completed\")\n",
    "    print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "    print(f\"VoID triples: {len(complete_void_graph):,}\")\n",
    "    print(f\"Output file:\", output_path)\n",
    "\n",
    "    # Extract schema with full statistics\n",
    "    complete_parser = VoidParser(complete_void_graph)\n",
    "    complete_schema_df = complete_parser.to_schema(filter_void_nodes=True)\n",
    "\n",
    "    print(f\"\\nComplete Schema Analysis:\")\n",
    "    print(f\"  Schema triples: {len(complete_schema_df):,}\")\n",
    "    print(f\"  Unique classes: {complete_schema_df['subject_class'].nunique()}\")\n",
    "    print(f\"  Unique properties: {complete_schema_df['property'].nunique()}\")\n",
    "    print(f\"  Unique objects: {complete_schema_df['object_class'].nunique()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR in complete VoID generation: {e}\")\n",
    "    print(\"Full traceback:\")\n",
    "    traceback.print_exc()\n",
    "    complete_void_graph = None\n",
    "    complete_parser = None\n",
    "    complete_schema_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed4057",
   "metadata": {},
   "source": [
    "## 3. Fallback Extraction Mode (Python Post-Processing)\n",
    "\n",
    "Simpler SPARQL querying, process stats in Python. Preserves original values instead of Resource/Literal classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3085e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"=== SIMPLE EXTRACTION MODE (PYTHON POST-PROCESSING) ===\")\n",
    "    output_path = os.path.join(exports_path, f\"{dataset_name}_simple_void.ttl\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # simple extraction with Python post-processing - using generate_void_from_sparql directly\n",
    "    simple_void_graph = VoidParser.generate_void_from_sparql(\n",
    "        endpoint_url=endpoint_url,\n",
    "        graph_uri=graph_uri,\n",
    "        output_file=output_path,\n",
    "        counts=False,        # Disable counts for speed\n",
    "        #sample_limit=5000    # Use sampling for demonstration\n",
    "    )\n",
    "\n",
    "    # Create parser from the generated graph\n",
    "    simple_parser = VoidParser(simple_void_graph)\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    print(f\"simple extraction completed\")\n",
    "    print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "    print(f\"Processing: Python post-processing enabled\")\n",
    "    # print(f\"Sample limit: 5,000 triples per query\")\n",
    "\n",
    "    # Extract schema with preserved values\n",
    "    simple_schema_df = simple_parser.to_schema(filter_void_nodes=True)\n",
    "\n",
    "    print(f\"\\nsimple Extraction Schema Analysis:\")\n",
    "    print(f\"  Schema triples: {len(simple_schema_df):,}\")\n",
    "    print(f\"  Unique classes: {simple_schema_df['subject_class'].nunique()}\")\n",
    "    print(f\"  Unique properties: {simple_schema_df['property'].nunique()}\")\n",
    "    print(f\"  Unique objects: {simple_schema_df['object_class'].nunique()}\")\n",
    "    print(f\"Output file:\", output_path)\n",
    "\n",
    "    # Show detailed object types (preserved values)\n",
    "    if not simple_schema_df.empty:\n",
    "        preserved_objects = simple_schema_df[\n",
    "            ~simple_schema_df['object_class'].isin(['Resource', 'Literal'])\n",
    "        ]\n",
    "        print(f\"  Preserved object types: {len(preserved_objects):,} (vs Resource/Literal classification)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR in simple extraction: {e}\")\n",
    "    print(\"Full traceback:\")\n",
    "    traceback.print_exc()\n",
    "    simple_void_graph = None\n",
    "    simple_parser = None\n",
    "    simple_schema_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f5f39",
   "metadata": {},
   "source": [
    "## 4. Class Partition Coverage Analysis\n",
    "\n",
    "Modular analysis of class partition usage and coverage (counting instances for each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bdab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"=== CLASS PARTITION COVERAGE ANALYSIS ===\")\n",
    "\n",
    "    # Use the best available parser for coverage analysis\n",
    "    analysis_parser = None\n",
    "    if 'simple_parser' in globals() and simple_parser is not None:\n",
    "        analysis_parser = simple_parser\n",
    "        output_path = os.path.join(exports_path, f\"{dataset_name}_coverage.csv\")\n",
    "        print(\"Using simple_parser for coverage analysis...\")\n",
    "    elif 'fast_parser' in globals() and fast_parser is not None:\n",
    "        analysis_parser = fast_parser\n",
    "        print(\"Using fast_parser for coverage analysis...\")\n",
    "    elif 'sampled_parser' in globals() and sampled_parser is not None:\n",
    "        analysis_parser = sampled_parser\n",
    "        print(\"Using sampled_parser for coverage analysis...\")\n",
    "    elif 'complete_parser' in globals() and complete_parser is not None:\n",
    "        analysis_parser = complete_parser\n",
    "        print(\"Using complete_parser for coverage analysis...\")\n",
    "    else:\n",
    "        print(\"No parser available for coverage analysis. Please run previous cells first.\")\n",
    "        analysis_parser = None\n",
    "\n",
    "    if analysis_parser:\n",
    "        try:\n",
    "            print(\"Running class partition coverage analysis...\")\n",
    "\n",
    "            # Complete analysis pipeline\n",
    "            instance_counts, class_mappings, coverage_stats = analysis_parser.analyze_class_partition_usage(\n",
    "                endpoint_url=endpoint_url,\n",
    "                graph_uri=graph_uri,\n",
    "                #sample_limit=10000  # Sample for demonstration\n",
    "            )\n",
    "\n",
    "            print(f\"\\nCoverage Analysis Results:\")\n",
    "            print(f\"  Unique instances analyzed: {len(instance_counts):,}\")\n",
    "            print(f\"  Class partitions found: {len(class_mappings)}\")\n",
    "            print(f\"  Coverage statistics generated: {len(coverage_stats)}\")\n",
    "\n",
    "            # Export coverage analysis\n",
    "            coverage_df = analysis_parser.export_coverage_analysis(\n",
    "                coverage_stats, output_file=output_path\n",
    "            )\n",
    "            print(f\"Output file:\", output_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Coverage analysis failed: {e}\")\n",
    "            print(\"Full traceback:\")\n",
    "            traceback.print_exc()\n",
    "            coverage_df = None\n",
    "    else:\n",
    "        coverage_df = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ERROR in coverage analysis setup: {e}\")\n",
    "    print(\"Full traceback:\")\n",
    "    traceback.print_exc()\n",
    "    coverage_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7674c701",
   "metadata": {},
   "source": [
    "## 5. Schema Analysis and Comparison\n",
    "\n",
    "Compare results across different extraction modes and analyze schema patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f7999",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"=== SCHEMA ANALYSIS AND COMPARISON ===\")\n",
    "\n",
    "    # Collect available schemas for comparison\n",
    "    schemas = {}\n",
    "    if 'complete_schema_df' in globals() and complete_schema_df is not None:\n",
    "        schemas['Complete (with counts)'] = complete_schema_df\n",
    "    if 'fast_schema_df' in globals() and fast_schema_df is not None:\n",
    "        schemas['Fast (no counts)'] = fast_schema_df\n",
    "    if 'sampled_schema_df' in globals() and sampled_schema_df is not None:\n",
    "        schemas['Sampled (limited)'] = sampled_schema_df\n",
    "    if 'simple_schema_df' in globals() and simple_schema_df is not None:\n",
    "        schemas['simple (Python processed)'] = simple_schema_df\n",
    "\n",
    "    print(f\"Available schemas for comparison: {len(schemas)}\")\n",
    "\n",
    "    if schemas:\n",
    "        print(\"\\nMode Comparison:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Mode':<25} {'Triples':<10} {'Classes':<10} {'Properties':<12} {'Objects':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for mode_name, schema_df in schemas.items():\n",
    "            print(f\"{mode_name:<25} {len(schema_df):<10,} {schema_df['subject_class'].nunique():<10} \"\n",
    "                  f\"{schema_df['property'].nunique():<12} {schema_df['object_class'].nunique():<10}\")\n",
    "        \n",
    "        # Focus analysis on a representative schema\n",
    "        main_schema = list(schemas.values())[0]\n",
    "        print(f\"\\nDetailed Analysis ({list(schemas.keys())[0]}):\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Top classes by frequency\n",
    "        top_classes = main_schema['subject_class'].value_counts().head(10)\n",
    "        print(f\"\\nTop 10 Classes by Triple Count:\")\n",
    "        for class_name, count in top_classes.items():\n",
    "            print(f\"  {class_name:<30}: {count:,} triples\")\n",
    "        \n",
    "        # Top properties by frequency\n",
    "        top_properties = main_schema['property'].value_counts().head(10)\n",
    "        print(f\"\\nTop 10 Properties by Usage:\")\n",
    "        for prop_name, count in top_properties.items():\n",
    "            print(f\"  {prop_name:<30}: {count:,} usages\")\n",
    "        \n",
    "        # Object type distribution\n",
    "        object_dist = main_schema['object_class'].value_counts().head(10)\n",
    "        print(f\"\\nObject Type Distribution:\")\n",
    "        for obj_type, count in object_dist.items():\n",
    "            print(f\"  {obj_type:<30}: {count:,} instances\")\n",
    "\n",
    "        # KeyEvent class analysis (AOP-Wiki specific)\n",
    "        if 'KeyEvent' in main_schema['subject_class'].values:\n",
    "            ke_schema = main_schema[main_schema['subject_class'] == 'KeyEvent']\n",
    "            print(f\"\\nKeyEvent Class Analysis:\")\n",
    "            print(f\"  Properties: {len(ke_schema)}\")\n",
    "            print(f\"  Unique properties: {ke_schema['property'].nunique()}\")\n",
    "            \n",
    "            print(f\"\\n  KeyEvent Properties:\")\n",
    "            for _, row in ke_schema.head(10).iterrows():\n",
    "                print(f\"    {row['property']:<25} -> {row['object_class']}\")\n",
    "    else:\n",
    "        print(\"No schemas available for comparison\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ERROR in schema analysis and comparison: {e}\")\n",
    "    print(\"Full traceback:\")\n",
    "    traceback.print_exc()\n",
    "    schemas = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153b673",
   "metadata": {},
   "source": [
    "## 6. Export Formats and Serializations\n",
    "\n",
    "Generate exports in multiple formats: CSV, JSON, JSON-LD with automatic prefix extraction (`bioregistry`'s `curie_from_iri`' )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"=== EXPORT FORMATS AND SERIALIZATIONS ===\")\n",
    "\n",
    "    # Export schemas in multiple formats\n",
    "    export_count = 0\n",
    "\n",
    "    for mode_name, schema_df in schemas.items():\n",
    "        if schema_df is not None and len(schema_df) > 0:\n",
    "            try:\n",
    "                safe_name = mode_name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "                \n",
    "                # CSV Export - use exports_path\n",
    "                csv_file = os.path.join(exports_path, f\"{dataset_name}_{safe_name}_schema.csv\")\n",
    "                schema_df.to_csv(csv_file, index=False)\n",
    "                print(f\"CSV exported: {csv_file} ({len(schema_df):,} rows)\")\n",
    "                \n",
    "                # JSON Export (using appropriate parser)\n",
    "                if 'complete_parser' in globals() and mode_name.startswith('Complete'):\n",
    "                    parser = complete_parser\n",
    "                elif 'fast_parser' in globals() and mode_name.startswith('Fast'):\n",
    "                    parser = fast_parser\n",
    "                elif 'sampled_parser' in globals() and mode_name.startswith('Sampled'):\n",
    "                    parser = sampled_parser\n",
    "                elif 'simple_parser' in globals() and mode_name.startswith('Simple'):\n",
    "                    parser = simple_parser\n",
    "                else:\n",
    "                    parser = fast_parser if 'fast_parser' in globals() else None\n",
    "                    \n",
    "                if parser:\n",
    "                    schema_json = parser.to_json(filter_void_nodes=True)\n",
    "                    json_file = os.path.join(exports_path, f\"{dataset_name}_{safe_name}_schema.json\")\n",
    "                    \n",
    "                    with open(json_file, 'w') as f:\n",
    "                        json.dump(schema_json, f, indent=2)\n",
    "                    print(f\"JSON exported: {json_file} ({len(schema_json['triples']):,} triples)\")\n",
    "                \n",
    "                export_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"ERROR exporting {mode_name}: {e}\")\n",
    "                print(\"Full traceback:\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "    print(f\"\\nTotal exports generated: {export_count * 2} files\")\n",
    "\n",
    "    # JSON-LD Export with automatic prefix extraction\n",
    "    try:\n",
    "        print(f\"\\nJSON-LD Export with Automatic Prefix Extraction:\")\n",
    "\n",
    "        # Export VoID as JSON-LD - use exports_path\n",
    "        void_jsonld_file = os.path.join(exports_path, f\"{dataset_name}_void.jsonld\")\n",
    "        void_jsonld = solver.export_void_jsonld(\n",
    "            output_file=void_jsonld_file,\n",
    "            indent=2\n",
    "        )\n",
    "\n",
    "        # Export schema as JSON-LD - use exports_path\n",
    "        schema_jsonld_file = os.path.join(exports_path, f\"{dataset_name}_schema.jsonld\")\n",
    "        schema_jsonld = solver.export_schema_jsonld(\n",
    "            output_file=schema_jsonld_file,\n",
    "            indent=2,\n",
    "            filter_void_nodes=True\n",
    "        )\n",
    "\n",
    "        print(f\"  VoID JSON-LD: {void_jsonld_file} ({len(void_jsonld):,} characters)\")\n",
    "        print(f\"  Schema JSON-LD: {schema_jsonld_file} ({len(schema_jsonld):,} characters)\")\n",
    "\n",
    "        # Show automatically extracted prefixes\n",
    "        prefixes = solver._extract_prefixes_from_void()\n",
    "        print(f\"  Auto-extracted prefixes: {len(prefixes)}\")\n",
    "\n",
    "        prefix_list = list(prefixes.keys())[:10]\n",
    "        print(f\"  Sample prefixes: {', '.join(prefix_list)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in JSON-LD export: {e}\")\n",
    "        print(\"Full traceback:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Coverage Analysis Export\n",
    "    try:\n",
    "        if 'coverage_df' in globals() and coverage_df is not None:\n",
    "            coverage_file = os.path.join(exports_path, f\"{dataset_name}_coverage_analysis.csv\")\n",
    "            print(f\"\\nClass Partition Coverage Analysis:\")\n",
    "            print(f\"  Coverage CSV: {coverage_file} ({len(coverage_df)} class partitions)\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in coverage export: {e}\")\n",
    "        print(\"Full traceback:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(f\"\\nAll exports completed for dataset: {dataset_name}\")\n",
    "    print(f\"All files saved to: {exports_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR in export formats and serializations: {e}\")\n",
    "    print(\"Full traceback:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8566e6e4",
   "metadata": {},
   "source": [
    "## 7. Data Visualization and Final Analysis\n",
    "\n",
    "Display schema samples and provide summary of all analysis modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"=== RESULTS ===\")\n",
    "\n",
    "    # Display sample schemas\n",
    "    if schemas:\n",
    "        try:\n",
    "            # Show representative sample from the main schema\n",
    "            main_schema_name, main_schema = list(schemas.items())[0]\n",
    "            print(f\"\\nSchema Sample ({main_schema_name}):\")\n",
    "            \n",
    "            # Filter out generic classes and show meaningful relationships\n",
    "            sample_schema = main_schema[\n",
    "                ~main_schema['object_class'].isin(['Class', 'Resource', 'Literal'])\n",
    "            ].head(15)\n",
    "            \n",
    "            if len(sample_schema) > 0:\n",
    "                print(\"Subject Class                 | Property              | Object Class\")\n",
    "                print(\"-\" * 75)\n",
    "                for _, row in sample_schema.iterrows():\n",
    "                    subj = row['subject_class'][:25].ljust(25)\n",
    "                    prop = row['property'][:20].ljust(20)\n",
    "                    obj = row['object_class'][:25]\n",
    "                    print(f\"{subj} | {prop} | {obj}\")\n",
    "            else:\n",
    "                # Fallback to full sample\n",
    "                sample_schema = main_schema.head(10)\n",
    "                print(\"Subject Class                 | Property              | Object Class\")\n",
    "                print(\"-\" * 75)\n",
    "                for _, row in sample_schema.iterrows():\n",
    "                    subj = row['subject_class'][:25].ljust(25)\n",
    "                    prop = row['property'][:20].ljust(20)\n",
    "                    obj = row['object_class'][:25]\n",
    "                    print(f\"{subj} | {prop} | {obj}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR displaying schema samples: {e}\")\n",
    "            print(\"Full traceback:\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # Display coverage analysis if available\n",
    "    try:\n",
    "        if 'coverage_df' in globals() and coverage_df is not None:\n",
    "            print(f\"\\nClass Partition Coverage Analysis:\")\n",
    "            print(\"Class Name                    | Instances | Coverage% | Avg/Instance\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            for _, row in coverage_df.head(5).iterrows():\n",
    "                name = row['class_name'][:25].ljust(25)\n",
    "                instances = f\"{row['total_instances']:,}\".rjust(8)\n",
    "                coverage = f\"{row['occurrence_coverage_percent']:.1f}%\".rjust(8)\n",
    "                avg = f\"{row['avg_occurrences_per_instance']:.1f}\".rjust(10)\n",
    "                print(f\"{name} | {instances} | {coverage} | {avg}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR displaying coverage analysis: {e}\")\n",
    "        print(\"Full traceback:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    try:\n",
    "        workflow_summary = {\n",
    "            \"Traditional VoID\": \"Complete COUNT aggregations - comprehensive but slower\",\n",
    "            \"Fast Discovery\": \"No COUNT aggregations - rapid schema exploration\", \n",
    "            \"Sampled Analysis\": \"LIMIT clause - ultra-fast for large datasets\",\n",
    "            \"simple Extraction\": \"Python post-processing - preserves original values\",\n",
    "            \"Coverage Analysis\": \"Instance counting and class partition percentages\",\n",
    "            \"Export Formats\": \"CSV, JSON, JSON-LD with automatic prefixes\"\n",
    "        }\n",
    "\n",
    "        for workflow, description in workflow_summary.items():\n",
    "            status = \"✓\" if any(workflow.lower().replace(' ', '_') in str(k).lower() \n",
    "                           for k in globals().keys()) else \"○\"\n",
    "            print(f\"{status} {workflow:<20}: {description}\")\n",
    "\n",
    "        print(f\"\\nDataset: {dataset_name}\")\n",
    "        print(f\"Endpoint: {endpoint_url}\")\n",
    "        print(f\"Total analysis modes demonstrated: {len([k for k in globals().keys() if 'schema_df' in k])}\")\n",
    "\n",
    "        if schemas:\n",
    "            total_triples = sum(len(df) for df in schemas.values())\n",
    "            total_classes = sum(df['subject_class'].nunique() for df in schemas.values())\n",
    "            total_properties = sum(df['property'].nunique() for df in schemas.values())\n",
    "            \n",
    "            print(f\"Combined schema statistics:\")\n",
    "            print(f\"  Total schema triples across all modes: {total_triples:,}\")\n",
    "            print(f\"  Unique classes identified: {total_classes:,}\")\n",
    "            print(f\"  Unique properties identified: {total_properties:,}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR in summary generation: {e}\")\n",
    "        print(\"Full traceback:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR in results section: {e}\")\n",
    "    print(\"Full traceback:\")\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdfsolve-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
