{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d16b10",
   "metadata": {},
   "source": [
    "# hp Schema Extraction\n",
    "\n",
    "This notebook demonstrates RDF schema extraction from the hp SPARQL endpoint. It discovers VoID (Vocabulary of Interlinked Datasets) descriptions and generates  JSON-LD as the source for all downstream outputs including frequency analysis and LinkML schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5492f",
   "metadata": {},
   "source": [
    "## Exports\n",
    "\n",
    "- [JSON-LD Schema](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/data/schema_extraction/hp_schema/hp_schema.jsonld) (primary output)\n",
    "- [N-Quads RDF](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/data/schema_extraction/hp_schema/hp_schema.nq)\n",
    "- [VoID Graph](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/data/schema_extraction/hp_schema/hp_generated_void.ttl) for the dataset in its original source\n",
    "- [Coverage report](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/data/schema_extraction/hp_schema/hp_pattern_coverage.csv)\n",
    "- [LinkML Schema](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/data/schema_extraction/hp_schema/hp_linkml_schema.yaml)\n",
    "- [Full parquet entity dataframe](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/data/schema_extraction/hp_schema/hpinstances.parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ded37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Configuration\n",
    "import os\n",
    "\n",
    "# Dataset parameters\n",
    "endpoint_url = \"https://bio2rdf.org/sparql\"\n",
    "dataset_name = \"hp\"\n",
    "void_iri = \"http://bio2rdf.org/hp_resource:bio2rdf.dataset.hp.R3\"\n",
    "graph_uri = \"http://bio2rdf.org/hp_resource:bio2rdf.dataset.hp.R3\"\n",
    "\n",
    "# Setup paths\n",
    "working_path = os.path.abspath(\"\")\n",
    "exports_path = os.path.join(\n",
    "    working_path, \"..\", \"..\", \"docs\", \"data\", \"schema_extraction\", dataset_name\n",
    ")\n",
    "os.makedirs(exports_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Minimal notebook logger using existing dataset_name\n",
    "logger = logging.getLogger(dataset_name or \"notebook\")\n",
    "logger.setLevel(logging.DEBUG)  # Set to DEBUG to see SPARQL queries\n",
    "\n",
    "# Also configure the rdfsolve.parser logger to see query details\n",
    "parser_logger = logging.getLogger(\"rdfsolve.parser\")\n",
    "parser_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Avoid adding duplicate handlers if the cell is re-run\n",
    "if not logger.handlers:\n",
    "    fmt = logging.Formatter(\"%(asctime)s %(levelname)s %(name)s: %(message)s\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    sh = logging.StreamHandler(sys.stdout)\n",
    "    sh.setLevel(logging.DEBUG)  # Set to DEBUG to see all logs\n",
    "    sh.setFormatter(fmt)\n",
    "    logger.addHandler(sh)\n",
    "\n",
    "    # Add the same handler to the parser logger\n",
    "    parser_logger.addHandler(sh)\n",
    "\n",
    "logger.info(f\"Logging configured for {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cfa4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "\n",
    "# Configure Plotly for HTML output\n",
    "import plotly.io as pio\n",
    "import plotly.offline as pyo\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from rdfsolve.parser import VoidParser\n",
    "\n",
    "# Set renderer to 'notebook' for Jupyter, but ensure HTML export works\n",
    "pio.renderers.default = \"notebook+plotly_mimetype\"\n",
    "\n",
    "# Initialize offline mode for Plotly\n",
    "pyo.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d243f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle caching utilities\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def save_cache(data, filename, cache_dir=None):\n",
    "    \"\"\"Save data to pickle cache.\"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(exports_path, \"cache\")\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    cache_path = os.path.join(cache_dir, f\"{filename}.pkl\")\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Cached data to: {cache_path}\")\n",
    "    return cache_path\n",
    "\n",
    "\n",
    "def load_cache(filename, cache_dir=None):\n",
    "    \"\"\"Load data from pickle cache if it exists.\"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(exports_path, \"cache\")\n",
    "\n",
    "    cache_path = os.path.join(cache_dir, f\"{filename}.pkl\")\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"Loaded cached data from: {cache_path}\")\n",
    "        return data\n",
    "    return None\n",
    "\n",
    "\n",
    "def cache_exists(filename, cache_dir=None):\n",
    "    \"\"\"Check if cache file exists.\"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(exports_path, \"cache\")\n",
    "\n",
    "    cache_path = os.path.join(cache_dir, f\"{filename}.pkl\")\n",
    "    return os.path.exists(cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache management utilities\n",
    "def list_cache_files(cache_dir=None):\n",
    "    \"\"\"List all cache files.\"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(exports_path, \"cache\")\n",
    "\n",
    "    if not os.path.exists(cache_dir):\n",
    "        print(\"No cache directory found\")\n",
    "        return []\n",
    "\n",
    "    cache_files = [f for f in os.listdir(cache_dir) if f.endswith(\".pkl\")]\n",
    "    print(f\"Cache directory: {cache_dir}\")\n",
    "    for f in cache_files:\n",
    "        file_path = os.path.join(cache_dir, f)\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"  {f} ({size_mb:.2f} MB)\")\n",
    "    return cache_files\n",
    "\n",
    "\n",
    "def clear_cache(filename=None, cache_dir=None):\n",
    "    \"\"\"Clear specific cache file or all cache.\"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(exports_path, \"cache\")\n",
    "\n",
    "    if filename:\n",
    "        cache_path = os.path.join(cache_dir, f\"{filename}.pkl\")\n",
    "        if os.path.exists(cache_path):\n",
    "            os.remove(cache_path)\n",
    "            print(f\"Removed cache: {filename}\")\n",
    "        else:\n",
    "            print(f\"Cache not found: {filename}\")\n",
    "    else:\n",
    "        # Clear all cache files\n",
    "        if os.path.exists(cache_dir):\n",
    "            import shutil\n",
    "\n",
    "            shutil.rmtree(cache_dir)\n",
    "            print(\"Cleared all cache files\")\n",
    "        else:\n",
    "            print(\"No cache directory to clear\")\n",
    "\n",
    "\n",
    "# Show current cache status\n",
    "list_cache_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd27dea",
   "metadata": {},
   "source": [
    "### Cache Control\n",
    "\n",
    "Use these cells to manage cached data. When testing new code changes, you may want to clear relevant cache files to force re-computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear specific cache files (uncomment lines as needed for testing)\n",
    "\n",
    "# When testing new parser changes:\n",
    "# clear_cache(f\"{dataset_name}_voidparser\")\n",
    "\n",
    "# When testing JSON-LD generation (primary output):\n",
    "# clear_cache(f\"{dataset_name}_jsonld_schema\")\n",
    "\n",
    "# When testing frequency calculations:\n",
    "# clear_cache(f\"{dataset_name}_frequencies_basic\")\n",
    "# clear_cache(f\"{dataset_name}_frequencies_with_instances\")\n",
    "\n",
    "# Clear everything:\n",
    "# clear_cache()\n",
    "\n",
    "print(\"Cleared parser and JSON-LD caches to test schema pattern fix\")\n",
    "print(\"Note: JSON-LD cache is now the primary cache - clear it when testing schema changes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce8103",
   "metadata": {},
   "source": [
    "## Discover or get VoID Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate VoID schema with caching\n",
    "cache_key = f\"{dataset_name}_voidparser\"\n",
    "\n",
    "# Try to load from cache first\n",
    "vp = load_cache(cache_key)\n",
    "\n",
    "if vp is None:\n",
    "    print(\"VoidParser not found in cache, generating...\")\n",
    "    vp = VoidParser.from_endpoint_with_discovery(\n",
    "        endpoint_url=endpoint_url,\n",
    "        dataset_name=dataset_name,\n",
    "        exports_path=exports_path,\n",
    "        # Arguments for the case when a suitable VoID is not found\n",
    "        graph_uris=[graph_uri] if graph_uri else None,\n",
    "        exclude_graph_patterns=[\n",
    "            \"openlinksw\",\n",
    "            \"well-known\",\n",
    "            \"void\",\n",
    "            \"service\",\n",
    "        ],  # Filter out service description and administrative graphs\n",
    "        counts=True,\n",
    "        offset_limit_steps=300,  # pagination\n",
    "    )\n",
    "    # Cache the VoidParser for future use\n",
    "    save_cache(vp, cache_key)\n",
    "else:\n",
    "    print(\"Loaded VoidParser from cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e848a08",
   "metadata": {},
   "source": [
    "## Schema Discovery and Exports Workflow\n",
    "\n",
    "### Workflow Steps:\n",
    "\n",
    "1. **VoID Discovery**: Extract schema patterns from SPARQL endpoint VoID descriptions\n",
    "2. **JSON-LD Generation**: Convert to JSON-LD.\n",
    "3. **Derived Outputs**: All other formats are generated from the JSON-LD structure:\n",
    "   - **Frequencies**: Schema pattern coverage analysis\n",
    "   - **LinkML**: LinkML YAML used elsewhere for other features.\n",
    "   - **CSV/JSON**: Tabular and structured data exports\n",
    "   - **RDF**: N-Quads serialization for triplestore import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b09ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary JSON-LD schema export and basic summary\n",
    "cache_key = f\"{dataset_name}_jsonld_schema\"\n",
    "jsonld_schema = load_cache(cache_key)\n",
    "\n",
    "if jsonld_schema is None:\n",
    "    print(\"Generating standards-compliant JSON-LD schema...\")\n",
    "    jsonld_schema = vp.to_jsonld(filter_void_admin_nodes=True)\n",
    "    save_cache(jsonld_schema, cache_key)\n",
    "else:\n",
    "    print(\"Loaded JSON-LD schema from cache\")\n",
    "\n",
    "# Save JSON-LD schema file\n",
    "jsonld_file = os.path.join(exports_path, f\"{dataset_name}_schema.jsonld\")\n",
    "with open(jsonld_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(jsonld_schema, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"JSON-LD Schema saved to: {jsonld_file}\")\n",
    "\n",
    "# Display combined JSON-LD structure info and schema summary\n",
    "if \"@graph\" in jsonld_schema:\n",
    "    print(\"\\nSchema Summary:\")\n",
    "    print(f\"   • Prefixes: {len(jsonld_schema['@context'])}\")\n",
    "    print(f\"   • Resources: {len(jsonld_schema['@graph'])}\")\n",
    "\n",
    "    # Show dataset metadata\n",
    "    dataset_info = jsonld_schema[\"@graph\"][0] if jsonld_schema[\"@graph\"] else {}\n",
    "    if dataset_info.get(\"@type\") == \"void:Dataset\":\n",
    "        print(f\"   • Dataset: {dataset_info.get('dcterms:title', 'Unknown')}\")\n",
    "        print(f\"   • Classes: {dataset_info.get('void:classes', 0)}\")\n",
    "        print(f\"   • Properties: {dataset_info.get('void:properties', 0)}\")\n",
    "        print(f\"   • Triples: {dataset_info.get('void:triples', 0)}\")\n",
    "\n",
    "# Get schema DataFrame and show sample\n",
    "schema_df = vp.to_schema(filter_void_admin_nodes=True)\n",
    "print(f\"\\nSchema Patterns Preview ({len(schema_df)} total):\")\n",
    "display(schema_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a530dd",
   "metadata": {},
   "source": [
    "## Schema Pattern Coverage Analysis\n",
    "Calculate coverage ratios showing what percentage of entities use each relationship pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b864b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema pattern coverage analysis and export\n",
    "cache_key = f\"{dataset_name}_frequencies_basic\"\n",
    "cached_data = load_cache(cache_key)\n",
    "\n",
    "if cached_data is None:\n",
    "    print(\"Calculating schema pattern frequencies...\")\n",
    "    frequencies_df, _ = vp.count_schema_shape_frequencies(\n",
    "        endpoint_url=endpoint_url,\n",
    "        offset_limit_steps=300,\n",
    "    )\n",
    "    save_cache(frequencies_df, cache_key)\n",
    "else:\n",
    "    print(\"Loaded frequencies DataFrame from cache\")\n",
    "    frequencies_df = cached_data\n",
    "\n",
    "# Export coverage analysis\n",
    "frequencies_output_path = os.path.join(exports_path, f\"{dataset_name}_pattern_coverage.csv\")\n",
    "exported_df = vp.export_schema_shape_frequencies(\n",
    "    frequencies_df, output_file=frequencies_output_path\n",
    ")\n",
    "\n",
    "# Combined summary and sample\n",
    "if not frequencies_df.empty:\n",
    "    avg_coverage = frequencies_df[\"coverage_percent\"].mean()\n",
    "    high_coverage = (frequencies_df[\"coverage_percent\"] > 50).sum()\n",
    "\n",
    "    print(\"\\nPattern Coverage Analysis:\")\n",
    "    print(f\"   • Total patterns: {len(frequencies_df)}\")\n",
    "    print(f\"   • Average coverage: {avg_coverage:.1f}%\")\n",
    "    print(f\"   • High coverage (>50%): {high_coverage}\")\n",
    "    print(f\"   • Exported to: {frequencies_output_path}\")\n",
    "\n",
    "    print(\"\\nSample Coverage Data:\")\n",
    "    display(\n",
    "        frequencies_df[[\"subject_class\", \"property\", \"object_class\", \"coverage_percent\"]].head()\n",
    "    )\n",
    "\n",
    "    print(\"\\nCoverage Statistics:\")\n",
    "    display(frequencies_df[\"coverage_percent\"].describe())\n",
    "else:\n",
    "    print(\"No frequency data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a15c1",
   "metadata": {},
   "source": [
    "## Schema Pattern Instance Collection\n",
    "Collect actual subject and object IRI instances for each schema pattern. This provides detailed access to the specific entities participating in each relationship pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5833f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect both frequency data and actual instances with caching\n",
    "cache_key = f\"{dataset_name}_frequencies_with_instances\"\n",
    "cached_data = load_cache(cache_key)\n",
    "\n",
    "if cached_data is None:\n",
    "    print(\"Collecting frequency data and instances...\")\n",
    "    frequencies_with_instances_df, instances_df = vp.count_schema_shape_frequencies(\n",
    "        endpoint_url=endpoint_url,\n",
    "        # sample_limit=100,  # Limited sample for demonstration\n",
    "        collect_instances=True,\n",
    "        offset_limit_steps=300,\n",
    "    )\n",
    "    # Cache both DataFrames as a tuple\n",
    "    save_cache((frequencies_with_instances_df, instances_df), cache_key)\n",
    "else:\n",
    "    print(\"Loaded frequencies and instances DataFrames from cache\")\n",
    "    frequencies_with_instances_df, instances_df = cached_data\n",
    "\n",
    "# Display basic information about the data structure\n",
    "print(f\"Frequencies DataFrame: {len(frequencies_with_instances_df)} shapes\")\n",
    "if instances_df is not None:\n",
    "    print(f\"Instances DataFrame: {len(instances_df)} subject-object pairs\")\n",
    "    print(\n",
    "        f\"Memory usage - Frequencies: {frequencies_with_instances_df.memory_usage(deep=True).sum() / 1024:.1f} KB\"\n",
    "    )\n",
    "    print(f\"Memory usage - Instances: {instances_df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "else:\n",
    "    print(\"No instances collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db811da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "if not frequencies_df.empty:\n",
    "    df = frequencies_df.copy()\n",
    "    df[\"coverage_percent\"] = pd.to_numeric(df[\"coverage_percent\"], errors=\"coerce\").fillna(0)\n",
    "    df = df.sort_values(\"coverage_percent\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    def make_label(row):\n",
    "        return (\n",
    "            f\"<b>{row['subject_class']}</b> \"\n",
    "            f\"<span style='color:#888;'></span> \"\n",
    "            f\"<i>{row['property']}</i> \"\n",
    "            f\"<span style='color:#888;'></span> \"\n",
    "            f\"<b>{row['object_class']}</b>\"\n",
    "        )\n",
    "\n",
    "    df[\"styled_label\"] = df.apply(make_label, axis=1)\n",
    "\n",
    "    text_positions = [\"outside\" if v < 95 else \"inside\" for v in df[\"coverage_percent\"]]\n",
    "    custom_colorscale = [\n",
    "        [0.0, \"#d36e61\"],\n",
    "        [0.4, \"#e5cdbd\"],\n",
    "        [0.7, \"#e8e4cf\"],\n",
    "        [1.0, \"#c3d9c0\"],\n",
    "    ]\n",
    "\n",
    "    # Figure sizing\n",
    "    bar_height = 26\n",
    "    fig_height = min(2000, bar_height * len(df) + 200)\n",
    "\n",
    "    fig = go.Figure(\n",
    "        go.Bar(\n",
    "            x=df[\"coverage_percent\"],\n",
    "            y=df[\"styled_label\"],\n",
    "            orientation=\"h\",\n",
    "            text=[f\"{v:.1f}%\" for v in df[\"coverage_percent\"]],\n",
    "            textposition=text_positions,\n",
    "            marker={\n",
    "                \"color\": df[\"coverage_percent\"],\n",
    "                \"colorscale\": custom_colorscale,\n",
    "                \"cmin\": 0,\n",
    "                \"cmax\": 100,\n",
    "                \"line\": {\"color\": \"white\", \"width\": 0.6},\n",
    "            },\n",
    "            hovertemplate=\"<b>%{y}</b><br>Coverage: %{x:.1f}%<extra></extra>\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            \"text\": f\"Schema Pattern Coverage for {dataset_name}\",\n",
    "            \"x\": 0.5,\n",
    "            \"font\": {\"size\": 18},\n",
    "        },\n",
    "        xaxis={\n",
    "            \"title\": \"Coverage (%)\",\n",
    "            \"range\": [0, 100],  # fixed x-axis range\n",
    "            \"ticksuffix\": \"%\",\n",
    "            \"showgrid\": True,\n",
    "            \"gridcolor\": \"rgba(220,220,220,0.3)\",\n",
    "        },\n",
    "        yaxis={\n",
    "            \"title\": \"\",\n",
    "            \"autorange\": \"reversed\",\n",
    "            \"automargin\": True,\n",
    "            \"fixedrange\": False,  # allow vertical zoom/pan\n",
    "        },\n",
    "        template=\"plotly_white\",\n",
    "        autosize=True,  # allow figure to scale with container\n",
    "        height=fig_height,  # base height (will scale)\n",
    "        margin={\"t\": 80, \"b\": 50, \"l\": 480, \"r\": 150},  # extra right margin for text\n",
    "        plot_bgcolor=\"white\",\n",
    "        paper_bgcolor=\"white\",\n",
    "    )\n",
    "\n",
    "    # Disable horizontal zoom/pan\n",
    "    fig.update_xaxes(fixedrange=True)\n",
    "\n",
    "    # Show figure with config for HTML export compatibility\n",
    "    fig.show(\n",
    "        config={\n",
    "            \"scrollZoom\": True,\n",
    "            \"responsive\": True,\n",
    "            \"toImageButtonOptions\": {\n",
    "                \"format\": \"png\",\n",
    "                \"filename\": f\"{dataset_name}_schema_coverage\",\n",
    "                \"height\": fig_height,\n",
    "                \"width\": 600,\n",
    "                \"scale\": 1,\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "else:\n",
    "    display(Markdown(\"**No coverage data to visualize**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9920263",
   "metadata": {},
   "source": [
    "## LinkML (derived from JSON-LD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2baa043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LinkML directly from JSON-LD with custom schema URI\n",
    "print(\"Regenerating LinkML schema from JSON-LD with custom schema URI...\")\n",
    "\n",
    "schema_name = f\"{dataset_name}_schema\"\n",
    "custom_schema_uri = (\n",
    "    f\"http://jmillanacosta.github.io/rdfsolve/{dataset_name}/linkml\"  # User-definable base URI\n",
    ")\n",
    "\n",
    "yaml_text = vp.to_linkml_yaml(\n",
    "    schema_name=schema_name,\n",
    "    schema_description=f\"LinkML schema for {dataset_name} generated from JSON-LD\",\n",
    "    schema_base_uri=custom_schema_uri,\n",
    "    filter_void_nodes=True,\n",
    ")\n",
    "\n",
    "# Save to LinkML YAML\n",
    "linkml_file = os.path.join(exports_path, f\"{dataset_name}_linkml_schema.yaml\")\n",
    "with open(linkml_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(yaml_text)\n",
    "\n",
    "print(f\"LinkML YAML saved to: {linkml_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkml.generators.erdiagramgen import ERDiagramGenerator\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "\n",
    "sv = SchemaView(linkml_file)\n",
    "linkml_schema = sv.schema\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"**Parsed LinkML schema:** Classes = {len(sv.all_classes())}, Slots = {len(sv.all_slots())}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build and display a Mermaid class diagram for the aopwikirdf LinkedML\n",
    "mermaid_code = ERDiagramGenerator(linkml_file).serialize()\n",
    "\n",
    "display(Markdown(mermaid_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = os.path.join(exports_path, f\"{dataset_name}_schema.json\")\n",
    "csv_path = os.path.join(exports_path, f\"{dataset_name}_schema.csv\")\n",
    "\n",
    "# Export CSV from frequencies\n",
    "frequencies_df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Export JSON derived from JSON-LD (maintains consistency)\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(vp.to_json(filter_void_nodes=True), fh, indent=2)\n",
    "\n",
    "print(f\"CSV exported to: {csv_path}\")\n",
    "print(f\"JSON exported to: {json_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
