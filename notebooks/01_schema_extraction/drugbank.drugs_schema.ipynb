{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d16b10",
   "metadata": {},
   "source": [
    "# drugbank.drugs Schema Extraction\n",
    "\n",
    "This notebook demonstrates RDF schema extraction from the drugbank.drugs SPARQL endpoint using a unified JSON-LD first approach. It discovers VoID (Vocabulary of Interlinked Datasets) descriptions and generates standards-compliant JSON-LD as the source for all downstream outputs including frequency analysis and LinkML schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5492f",
   "metadata": {},
   "source": [
    "## Exports\n",
    "\n",
    "- [JSON-LD Schema](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/notebooks/schema_extraction/drugbank.drugs_schema/drugbank.drugs_schema.jsonld) (primary output)\n",
    "- [N-Quads RDF](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/notebooks/schema_extraction/drugbank.drugs_schema/drugbank.drugs_schema.nq)\n",
    "- [VoID Graph](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/notebooks/schema_extraction/drugbank.drugs_schema/drugbank.drugs_generated_void.ttl) for the dataset in its original source\n",
    "- [Coverage report](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/notebooks/schema_extraction/drugbank.drugs_schema/drugbank.drugs_pattern_coverage.csv)\n",
    "- [LinkML Schema](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/notebooks/schema_extraction/drugbank.drugs_schema/drugbank.drugs_linkml_schema.yaml)\n",
    "- [Full parquet entity dataframe](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/notebooks/schema_extraction/drugbank.drugs_schema/drugbank.drugsinstances.parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ded37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Configuration\n",
    "import os\n",
    "\n",
    "# Dataset parameters\n",
    "endpoint_url = \"https://idsm.elixir-czech.cz/sparql/endpoint/idsm\"\n",
    "dataset_name = \"drugbank.drugs\"\n",
    "void_iri = \"http://wifo5-04.informatik.uni-mannheim.de/drugbank/resource/drugs/\"\n",
    "graph_uri = \"http://wifo5-04.informatik.uni-mannheim.de/drugbank/resource/drugs/\"\n",
    "\n",
    "# Setup paths\n",
    "working_path = os.path.abspath(\"\")\n",
    "exports_path = os.path.join(working_path, \"..\", \"..\", \"docs\", \"notebooks\", dataset_name)\n",
    "os.makedirs(exports_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Minimal notebook logger using existing dataset_name\n",
    "logger = logging.getLogger(dataset_name or \"notebook\")\n",
    "logger.setLevel(logging.DEBUG)  # Set to DEBUG to see SPARQL queries\n",
    "\n",
    "# Also configure the rdfsolve.parser logger to see query details\n",
    "parser_logger = logging.getLogger('rdfsolve.parser')\n",
    "parser_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Avoid adding duplicate handlers if the cell is re-run\n",
    "if not logger.handlers:\n",
    "    fmt = logging.Formatter(\"%(asctime)s %(levelname)s %(name)s: %(message)s\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    sh = logging.StreamHandler(sys.stdout)\n",
    "    sh.setLevel(logging.DEBUG)  # Set to DEBUG to see all logs\n",
    "    sh.setFormatter(fmt)\n",
    "    logger.addHandler(sh)\n",
    "    \n",
    "    # Add the same handler to the parser logger\n",
    "    parser_logger.addHandler(sh)\n",
    "\n",
    "logger.info(f\"Logging configured for {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cfa4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "from rdfsolve.parser import VoidParser\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Configure Plotly for HTML output\n",
    "import plotly.io as pio\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# Set renderer to 'notebook' for Jupyter, but ensure HTML export works\n",
    "pio.renderers.default = \"notebook+plotly_mimetype\"\n",
    "\n",
    "# Initialize offline mode for Plotly\n",
    "pyo.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d243f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle caching utilities\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def save_cache(data, filename, cache_dir=None):\n",
    "    \"\"\"Save data to pickle cache\"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(exports_path, \"cache\")\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    cache_path = os.path.join(cache_dir, f\"{filename}.pkl\")\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Cached data to: {cache_path}\")\n",
    "    return cache_path\n",
    "\n",
    "def load_cache(filename, cache_dir=None):\n",
    "    \"\"\"Load data from pickle cache if it exists\"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(exports_path, \"cache\")\n",
    "    \n",
    "    cache_path = os.path.join(cache_dir, f\"{filename}.pkl\")\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"Loaded cached data from: {cache_path}\")\n",
    "        return data\n",
    "    return None\n",
    "\n",
    "def cache_exists(filename, cache_dir=None):\n",
    "    \"\"\"Check if cache file exists\"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(exports_path, \"cache\")\n",
    "    \n",
    "    cache_path = os.path.join(cache_dir, f\"{filename}.pkl\")\n",
    "    return os.path.exists(cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache management utilities\n",
    "def list_cache_files(cache_dir=None):\n",
    "    \"\"\"List all cache files\"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(exports_path, \"cache\")\n",
    "    \n",
    "    if not os.path.exists(cache_dir):\n",
    "        print(\"No cache directory found\")\n",
    "        return []\n",
    "    \n",
    "    cache_files = [f for f in os.listdir(cache_dir) if f.endswith('.pkl')]\n",
    "    print(f\"Cache directory: {cache_dir}\")\n",
    "    for f in cache_files:\n",
    "        file_path = os.path.join(cache_dir, f)\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"  {f} ({size_mb:.2f} MB)\")\n",
    "    return cache_files\n",
    "\n",
    "def clear_cache(filename=None, cache_dir=None):\n",
    "    \"\"\"Clear specific cache file or all cache\"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(exports_path, \"cache\")\n",
    "    \n",
    "    if filename:\n",
    "        cache_path = os.path.join(cache_dir, f\"{filename}.pkl\")\n",
    "        if os.path.exists(cache_path):\n",
    "            os.remove(cache_path)\n",
    "            print(f\"Removed cache: {filename}\")\n",
    "        else:\n",
    "            print(f\"Cache not found: {filename}\")\n",
    "    else:\n",
    "        # Clear all cache files\n",
    "        if os.path.exists(cache_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(cache_dir)\n",
    "            print(f\"Cleared all cache files\")\n",
    "        else:\n",
    "            print(\"No cache directory to clear\")\n",
    "\n",
    "# Show current cache status\n",
    "list_cache_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd27dea",
   "metadata": {},
   "source": [
    "### Cache Control\n",
    "\n",
    "Use these cells to manage cached data. When testing new code changes, you may want to clear relevant cache files to force re-computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear specific cache files (uncomment lines as needed for testing)\n",
    "\n",
    "# When testing new parser changes:\n",
    "clear_cache(f\"{dataset_name}_voidparser\")\n",
    "\n",
    "# When testing JSON-LD generation (primary output):\n",
    "clear_cache(f\"{dataset_name}_jsonld_schema\")\n",
    "\n",
    "# When testing frequency calculations:\n",
    "# clear_cache(f\"{dataset_name}_frequencies_basic\")\n",
    "# clear_cache(f\"{dataset_name}_frequencies_with_instances\")\n",
    "\n",
    "# Clear everything:\n",
    "clear_cache()\n",
    "\n",
    "print(\"Cleared parser and JSON-LD caches to test schema pattern fix\")\n",
    "print(\"Note: JSON-LD cache is now the primary cache - clear it when testing schema changes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce8103",
   "metadata": {},
   "source": [
    "## Discover or get VoID Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate VoID schema with caching\n",
    "cache_key = f\"{dataset_name}_voidparser\"\n",
    "\n",
    "# Try to load from cache first\n",
    "vp = load_cache(cache_key)\n",
    "\n",
    "if vp is None:\n",
    "    print(\"VoidParser not found in cache, generating...\")\n",
    "    vp = VoidParser.from_endpoint_with_discovery(\n",
    "        endpoint_url=endpoint_url,\n",
    "        dataset_name=dataset_name,\n",
    "        exports_path=exports_path,\n",
    "        # Arguments for the case when a suitable VoID is not found\n",
    "        graph_uris=[graph_uri] if graph_uri else None,\n",
    "        exclude_graph_patterns=[\"openlinksw\",\n",
    "                                \"well-known\",\n",
    "                                \"void\",\n",
    "                                \"service\"], # Filter out service description and administrative graphs\n",
    "        counts=True,\n",
    "        offset_limit_steps=300 # pagination\n",
    "    )\n",
    "    # Cache the VoidParser for future use\n",
    "    save_cache(vp, cache_key)\n",
    "else:\n",
    "    print(\"Loaded VoidParser from cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e848a08",
   "metadata": {},
   "source": [
    "## Schema Discovery and Exports Workflow\n",
    "\n",
    "### Workflow Steps:\n",
    "\n",
    "1. **VoID Discovery**: Extract schema patterns from SPARQL endpoint VoID descriptions\n",
    "2. **JSON-LD Generation**: Convert to standards-compliant JSON-LD using established vocabularies:\n",
    "   - VoID (Vocabulary of Interlinked Datasets) for dataset descriptions\n",
    "   - SHACL for property shape patterns  \n",
    "   - Dublin Core Terms for metadata\n",
    "   - PROV-O for provenance information\n",
    "   - RDF/RDFS/OWL for core semantic types\n",
    "\n",
    "3. **Derived Outputs**: All other formats are generated from the JSON-LD structure:\n",
    "   - **Frequencies**: Schema pattern coverage analysis\n",
    "   - **LinkML**: LinkML YAML used elsewhere for other features.\n",
    "   - **CSV/JSON**: Tabular and structured data exports\n",
    "   - **RDF**: N-Quads serialization for triplestore import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b09ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary JSON-LD schema export (source of truth for all other formats)\n",
    "cache_key = f\"{dataset_name}_jsonld_schema\"\n",
    "jsonld_schema = load_cache(cache_key)\n",
    "\n",
    "if jsonld_schema is None:\n",
    "    print(\"Generating standards-compliant JSON-LD schema...\")\n",
    "    jsonld_schema = vp.to_jsonld(filter_void_admin_nodes=True)\n",
    "    save_cache(jsonld_schema, cache_key)\n",
    "else:\n",
    "    print(\"Loaded JSON-LD schema from cache\")\n",
    "\n",
    "# Save JSON-LD schema file\n",
    "jsonld_file = os.path.join(exports_path, f\"{dataset_name}_schema.jsonld\")\n",
    "with open(jsonld_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(jsonld_schema, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"JSON-LD Schema saved to: {jsonld_file}\")\n",
    "\n",
    "# Display JSON-LD structure info\n",
    "if \"@graph\" in jsonld_schema:\n",
    "    print(f\"Standards-compliant JSON-LD with {len(jsonld_schema['@context'])} prefixes and {len(jsonld_schema['@graph'])} resources\")\n",
    "    \n",
    "    # Show dataset metadata\n",
    "    dataset_info = jsonld_schema[\"@graph\"][0] if jsonld_schema[\"@graph\"] else {}\n",
    "    if dataset_info.get(\"@type\") == \"void:Dataset\":\n",
    "        print(f\"Dataset: {dataset_info.get('dcterms:title', 'Unknown')}\")\n",
    "        print(f\"Classes: {dataset_info.get('void:classes', 0)}\")\n",
    "        print(f\"Properties: {dataset_info.get('void:properties', 0)}\")\n",
    "        print(f\"Triples: {dataset_info.get('void:triples', 0)}\")\n",
    "\n",
    "# Basic schema DataFrame derived from JSON-LD\n",
    "schema_df = vp.to_schema(filter_void_admin_nodes=True)\n",
    "display(schema_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab4a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate JSON-LD with pyld (optional - requires pyld installation)\n",
    "try:\n",
    "    from pyld import jsonld as pyld_lib\n",
    "    \n",
    "    print(\"Validating JSON-LD compliance...\")\n",
    "    \n",
    "    # Test JSON-LD expansion (validates structure)\n",
    "    expanded = pyld_lib.expand(jsonld_schema)\n",
    "    print(f\"JSON-LD expansion successful: {len(expanded)} top-level items\")\n",
    "    \n",
    "    # Convert to RDF triples (validates semantic correctness)\n",
    "    nquads = pyld_lib.to_rdf(jsonld_schema, {'format': 'application/n-quads'})\n",
    "    triples_count = len(nquads.strip().split('\\n')) if nquads.strip() else 0\n",
    "    print(f\"RDF conversion successful: {triples_count} triples generated\")\n",
    "    \n",
    "    # Save N-Quads for reference\n",
    "    nquads_file = os.path.join(exports_path, f\"{dataset_name}_schema.nq\")\n",
    "    with open(nquads_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(nquads)\n",
    "    print(f\"N-Quads saved to: {nquads_file}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"pyld not available - skipping JSON-LD validation (install with: pip install pyld)\")\n",
    "except Exception as e:\n",
    "    print(f\"JSON-LD validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a530dd",
   "metadata": {},
   "source": [
    "## Schema Pattern Coverage Analysis (derived from JSON-LD)\n",
    "For each subject class type, calculate how many entities participate in each schema pattern divided by the total number of entities of that class type. This gives coverage ratios showing what percentage of entities actually use each relationship pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b864b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate schema pattern coverage ratios with caching\n",
    "cache_key = f\"{dataset_name}_frequencies_basic\"\n",
    "cached_data = load_cache(cache_key)\n",
    "\n",
    "if cached_data is None:\n",
    "    print(\"Calculating schema pattern frequencies...\")\n",
    "    frequencies_df, _ = vp.count_schema_shape_frequencies(\n",
    "        endpoint_url=endpoint_url,\n",
    "        offset_limit_steps=300,\n",
    "    )\n",
    "    save_cache(frequencies_df, cache_key)\n",
    "else:\n",
    "    print(\"Loaded frequencies DataFrame from cache\")\n",
    "    frequencies_df = cached_data\n",
    "\n",
    "frequencies_df[['subject_class', 'property', 'object_class', 'coverage_percent']].sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d82e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export coverage analysis\n",
    "frequencies_output_path = os.path.join(exports_path, f\"{dataset_name}_pattern_coverage.csv\")\n",
    "exported_df = vp.export_schema_shape_frequencies(frequencies_df, output_file=frequencies_output_path)\n",
    "\n",
    "# Simple summary\n",
    "if not frequencies_df.empty:\n",
    "    avg_coverage = frequencies_df['coverage_percent'].mean()\n",
    "    high_coverage = (frequencies_df['coverage_percent'] > 50).sum()\n",
    "    display(Markdown(f\"\"\"\n",
    "**Pattern Coverage Summary:**\n",
    "- Average pattern coverage: **{avg_coverage:.1f}%**\n",
    "- Patterns with >50% coverage: **{high_coverage}/{len(frequencies_df)}**\n",
    "- Exported to: `{frequencies_output_path}`\n",
    "- Derived from JSON-LD schema for consistency\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ca31c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a15c1",
   "metadata": {},
   "source": [
    "## Schema Pattern Instance Collection\n",
    "Collect actual subject and object IRI instances for each schema pattern. This provides detailed access to the specific entities participating in each relationship pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5833f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect both frequency data and actual instances with caching\n",
    "cache_key = f\"{dataset_name}_frequencies_with_instances\"\n",
    "cached_data = load_cache(cache_key)\n",
    "\n",
    "if cached_data is None:\n",
    "    print(\"Collecting frequency data and instances...\")\n",
    "    frequencies_with_instances_df, instances_df = vp.count_schema_shape_frequencies(\n",
    "        endpoint_url=endpoint_url,\n",
    "        #sample_limit=100,  # Limited sample for demonstration\n",
    "        collect_instances=True,\n",
    "        offset_limit_steps=300\n",
    "    )\n",
    "    # Cache both DataFrames as a tuple\n",
    "    save_cache((frequencies_with_instances_df, instances_df), cache_key)\n",
    "else:\n",
    "    print(\"Loaded frequencies and instances DataFrames from cache\")\n",
    "    frequencies_with_instances_df, instances_df = cached_data\n",
    "\n",
    "# Display basic information about the data structure\n",
    "print(f\"Frequencies DataFrame: {len(frequencies_with_instances_df)} shapes\")\n",
    "if instances_df is not None:\n",
    "    print(f\"Instances DataFrame: {len(instances_df)} subject-object pairs\")\n",
    "    print(f\"Memory usage - Frequencies: {frequencies_with_instances_df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "    print(f\"Memory usage - Instances: {instances_df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "else:\n",
    "    print(\"No instances collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e42ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the structure of the linked DataFrames\n",
    "if instances_df is not None:\n",
    "    print(\"Frequencies DataFrame columns:\")\n",
    "    print(list(frequencies_with_instances_df.columns))\n",
    "    print(\"\\nInstances DataFrame columns:\")\n",
    "    print(list(instances_df.columns))\n",
    "    print(f\"\\nInstances DataFrame dtypes (note categorical optimization):\")\n",
    "    print(instances_df.dtypes)\n",
    "    \n",
    "    # Show sample of frequencies data with shape_id for linking\n",
    "    print(\"\\nSample frequencies data:\")\n",
    "    display(frequencies_with_instances_df.head(3))\n",
    "    \n",
    "    # Show sample instances data\n",
    "    print(\"\\nSample instances data:\")\n",
    "    display(instances_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a34768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Filter shapes by coverage threshold and get their instances\n",
    "high_coverage_data = vp.get_shape_instances(\n",
    "    frequencies_with_instances_df, \n",
    "    instances_df,\n",
    "    #min_coverage=0.3\n",
    ")\n",
    "print(f\"{len(high_coverage_data)} instance records\")\n",
    "\n",
    "# Example 2: Direct DataFrame operations for specific analysis\n",
    "if not instances_df.empty:\n",
    "    # Count unique subjects per shape\n",
    "    subjects_per_shape = instances_df.groupby('shape_id')['subject_iri'].nunique()\n",
    "    print(f\"Shape with most unique subjects: {subjects_per_shape.max()} subjects\")\n",
    "    \n",
    "    # Find most common object types\n",
    "    common_objects = instances_df['object_iri'].value_counts().head(3)\n",
    "    print(f\"Most frequent object IRIs:\")\n",
    "    for iri, count in common_objects.items():\n",
    "        print(f\"  {iri}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5de390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution and memory characteristics\n",
    "if instances_df is not None:\n",
    "    distribution_analysis = vp.analyze_shape_distribution(frequencies_with_instances_df, instances_df)\n",
    "    \n",
    "    print(\"Distribution Analysis:\")\n",
    "    print(f\"  Total shapes: {distribution_analysis.get('total_shapes', 0)}\")\n",
    "    print(f\"  Shapes with instances: {distribution_analysis.get('shapes_with_instances', 0)}\")\n",
    "    print(f\"  Total instance records: {distribution_analysis.get('total_instance_records', 0)}\")\n",
    "    print(f\"  Average instances per shape: {distribution_analysis.get('avg_instances_per_shape', 0):.1f}\")\n",
    "    print(f\"  Maximum instances per shape: {distribution_analysis.get('max_instances_per_shape', 0)}\")\n",
    "    \n",
    "    memory_info = distribution_analysis.get('memory_usage_mb', {})\n",
    "    print(f\"\\nMemory Usage:\")\n",
    "    print(f\"  Frequencies DataFrame: {memory_info.get('frequencies_df', 0):.2f} MB\")\n",
    "    print(f\"  Instances DataFrame: {memory_info.get('instances_df', 0):.2f} MB\")\n",
    "    \n",
    "    # Show top shapes by instance count\n",
    "    top_shapes = distribution_analysis.get('top_shapes_by_instances', {})\n",
    "    if top_shapes:\n",
    "        print(f\"\\nTop shapes by instance count:\")\n",
    "        for shape_id, info in list(top_shapes.items())[:3]:\n",
    "            count = info.get('actual_instances', 0)\n",
    "            pattern = info.get('shape_pattern', 'Unknown')\n",
    "            coverage = info.get('coverage_ratio', 0)\n",
    "            print(f\"  {pattern[:50]}... ({count} instances, {coverage:.1%} coverage)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0474e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of accessing specific shape instances\n",
    "if instances_df is not None and not frequencies_with_instances_df.empty:\n",
    "    # Get instances for the fifth shape as an example\n",
    "    sample_shape_id = frequencies_with_instances_df.iloc[4]['shape_id']\n",
    "    sample_shape_pattern = frequencies_with_instances_df.iloc[4]['shape_pattern']\n",
    "    \n",
    "    # Filter instances for this specific shape\n",
    "    shape_instances = instances_df[instances_df['shape_id'] == sample_shape_id]\n",
    "    \n",
    "    print(f\"Sample shape: {sample_shape_pattern}\")\n",
    "    print(f\"Shape ID: {sample_shape_id}\")\n",
    "    print(f\"Instance count: {len(shape_instances)}\")\n",
    "    \n",
    "    if not shape_instances.empty:\n",
    "        print(\"\\nSample instances:\")\n",
    "        display(shape_instances.head(3))\n",
    "        \n",
    "        print(f\"\\nUnique subjects: {shape_instances['subject_iri'].nunique()}\")\n",
    "        print(f\"Unique objects: {shape_instances['object_iri'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccaf728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze self-referential instances\n",
    "if instances_df is not None and not instances_df.empty:\n",
    "    print(\"Analyzing self-referential instances:\")\n",
    "    \n",
    "    # Find cases where subject and object are identical\n",
    "    self_refs = instances_df[instances_df['subject_iri'] == instances_df['object_iri']]\n",
    "    \n",
    "    print(f\"Total instances: {len(instances_df)}\")\n",
    "    print(f\"Self-referential instances: {len(self_refs)}\")\n",
    "    print(f\"Percentage self-referential: {len(self_refs)/len(instances_df)*100:.1f}%\")\n",
    "    \n",
    "    if not self_refs.empty:\n",
    "        print(f\"\\nProperties involved in self-references:\")\n",
    "        self_ref_props = self_refs['property'].value_counts()\n",
    "        display(self_ref_props.head(10))\n",
    "        \n",
    "        print(f\"\\nSample self-referential cases:\")\n",
    "        display(self_refs[['subject_iri', 'object_iri', 'property', 'subject_class', 'object_class']].head())\n",
    "        \n",
    "        # Check if these are meaningful or problematic\n",
    "        meaningful_props = {'owl:sameAs', 'rdfs:seeAlso', 'skos:exactMatch', 'dc:identifier', 'foaf:primaryTopic'}\n",
    "        problematic_self_refs = self_refs[~self_refs['property'].isin(meaningful_props)]\n",
    "        \n",
    "        print(f\"\\nPotentially problematic self-references: {len(problematic_self_refs)}\")\n",
    "        if not problematic_self_refs.empty:\n",
    "            print(\"Properties that shouldn't be self-referential:\")\n",
    "            display(problematic_self_refs['property'].value_counts().head())\n",
    "            \n",
    "    # Check for potential data quality issues\n",
    "    print(f\"\\nData quality check:\")\n",
    "    print(f\"Unique subjects: {instances_df['subject_iri'].nunique()}\")\n",
    "    print(f\"Unique objects: {instances_df['object_iri'].nunique()}\")\n",
    "    print(f\"Unique properties: {instances_df['property'].nunique()}\")\n",
    "    \n",
    "    # Look for patterns in the problematic case you mentioned\n",
    "    identifier_cases = instances_df[instances_df['property'] == 'dc:identifier']\n",
    "    if not identifier_cases.empty:\n",
    "        print(f\"\\nAnalyzing dc:identifier cases:\")\n",
    "        print(f\"Total dc:identifier instances: {len(identifier_cases)}\")\n",
    "        id_self_refs = identifier_cases[identifier_cases['subject_iri'] == identifier_cases['object_iri']]\n",
    "        print(f\"dc:identifier self-references: {len(id_self_refs)}\")\n",
    "        if not id_self_refs.empty:\n",
    "            print(\"Sample dc:identifier self-references:\")\n",
    "            display(id_self_refs[['subject_iri', 'object_iri', 'subject_class', 'object_class']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db811da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "\n",
    "if not frequencies_df.empty:\n",
    "    df = frequencies_df.copy()\n",
    "    df[\"coverage_percent\"] = pd.to_numeric(\n",
    "        df[\"coverage_percent\"], errors=\"coerce\"\n",
    "    ).fillna(0)\n",
    "    df = df.sort_values(\"coverage_percent\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    def make_label(row):\n",
    "        return (\n",
    "            f\"<b>{row['subject_class']}</b> \"\n",
    "            f\"<span style='color:#888;'></span> \"\n",
    "            f\"<i>{row['property']}</i> \"\n",
    "            f\"<span style='color:#888;'></span> \"\n",
    "            f\"<b>{row['object_class']}</b>\"\n",
    "        )\n",
    "\n",
    "    df[\"styled_label\"] = df.apply(make_label, axis=1)\n",
    "\n",
    "    text_positions = [\"outside\" if v < 95 else \"inside\" for v in df[\"coverage_percent\"]]\n",
    "    custom_colorscale = [\n",
    "        [0.0, \"#d36e61\"],\n",
    "        [0.4, \"#e5cdbd\"],\n",
    "        [0.7, \"#e8e4cf\"],\n",
    "        [1.0, \"#c3d9c0\"],\n",
    "    ]\n",
    "\n",
    "    # Figure sizing\n",
    "    bar_height = 26\n",
    "    fig_height = min(2000, bar_height * len(df) + 200)\n",
    "\n",
    "    fig = go.Figure(\n",
    "        go.Bar(\n",
    "            x=df[\"coverage_percent\"],\n",
    "            y=df[\"styled_label\"],\n",
    "            orientation=\"h\",\n",
    "            text=[f\"{v:.1f}%\" for v in df[\"coverage_percent\"]],\n",
    "            textposition=text_positions,\n",
    "            marker=dict(\n",
    "                color=df[\"coverage_percent\"],\n",
    "                colorscale=custom_colorscale,\n",
    "                cmin=0,\n",
    "                cmax=100,\n",
    "                line=dict(color=\"white\", width=0.6),\n",
    "            ),\n",
    "            hovertemplate=\"<b>%{y}</b><br>Coverage: %{x:.1f}%<extra></extra>\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            \"text\": f\"Schema Pattern Coverage for {dataset_name}\",\n",
    "            \"x\": 0.5,\n",
    "            \"font\": {\"size\": 18},\n",
    "        },\n",
    "        xaxis=dict(\n",
    "            title=\"Coverage (%)\",\n",
    "            range=[0, 100],  # fixed x-axis range\n",
    "            ticksuffix=\"%\",\n",
    "            showgrid=True,\n",
    "            gridcolor=\"rgba(220,220,220,0.3)\",\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"\",\n",
    "            autorange=\"reversed\",\n",
    "            automargin=True,\n",
    "            fixedrange=False,  # allow vertical zoom/pan\n",
    "        ),\n",
    "        template=\"plotly_white\",\n",
    "        autosize=True,  # allow figure to scale with container\n",
    "        height=fig_height,  # base height (will scale)\n",
    "        margin=dict(t=80, b=50, l=480, r=150),  # extra right margin for text\n",
    "        plot_bgcolor=\"white\",\n",
    "        paper_bgcolor=\"white\",\n",
    "    )\n",
    "\n",
    "    # Disable horizontal zoom/pan\n",
    "    fig.update_xaxes(fixedrange=True)\n",
    "\n",
    "    # Show figure with config for HTML export compatibility\n",
    "    fig.show(config={\n",
    "        \"scrollZoom\": True, \n",
    "        \"responsive\": True,\n",
    "        \"toImageButtonOptions\": {\n",
    "            \"format\": \"png\",\n",
    "            \"filename\": f\"{dataset_name}_schema_coverage\",\n",
    "            \"height\": fig_height,\n",
    "            \"width\": 600,\n",
    "            \"scale\": 1\n",
    "        }\n",
    "    })\n",
    "\n",
    "else:\n",
    "    display(Markdown(\"**No coverage data to visualize**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9920263",
   "metadata": {},
   "source": [
    "## LinkML (derived from JSON-LD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2baa043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LinkML directly from JSON-LD with custom schema URI\n",
    "print(\"Regenerating LinkML schema from JSON-LD with custom schema URI...\")\n",
    "\n",
    "schema_name = f\"{dataset_name}_schema\"\n",
    "custom_schema_uri = f\"http://jmillanacosta.github.io/rdfsolve/{dataset_name}/linkml\"  # User-definable base URI\n",
    "\n",
    "yaml_text = vp.to_linkml_yaml(\n",
    "    schema_name=schema_name,\n",
    "    schema_description=f\"LinkML schema for {dataset_name} generated from JSON-LD\",\n",
    "    schema_base_uri=custom_schema_uri,\n",
    "    filter_void_nodes=True,\n",
    ")\n",
    "\n",
    "# Save to LinkML YAML\n",
    "linkml_file = os.path.join(exports_path, f\"{dataset_name}_linkml_schema.yaml\")\n",
    "with open(linkml_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(yaml_text)\n",
    "\n",
    "print(f\"LinkML YAML saved to: {linkml_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7429b5",
   "metadata": {},
   "source": [
    "### Mermaid  diagram for LinkML Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a4753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Mermaid diagram\n",
    "from linkml.generators.erdiagramgen import ERDiagramGenerator\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "    \n",
    "print(\"Generating Mermaid class diagram...\")\n",
    "    \n",
    " # Reload schema to ensure we have the latest version\n",
    "sv = SchemaView(linkml_file)\n",
    "linkml_schema = sv.schema\n",
    "\n",
    "display(Markdown(f\"**Parsed LinkML schema:** Classes = {len(sv.all_classes())}, Slots = {len(sv.all_slots())}\"))\n",
    "\n",
    "# Generate Mermaid diagram with error handling\n",
    "mermaid_code = ERDiagramGenerator(linkml_file).serialize()\n",
    "display(Markdown(mermaid_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = os.path.join(exports_path, f\"{dataset_name}_schema.json\")\n",
    "csv_path = os.path.join(exports_path, f\"{dataset_name}_schema.csv\")\n",
    "\n",
    "# Export CSV from frequencies\n",
    "frequencies_df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Export JSON derived from JSON-LD (maintains consistency)\n",
    "with open(json_path, 'w', encoding='utf-8') as fh:\n",
    "    json.dump(vp.to_json(filter_void_nodes=True), fh, indent=2)\n",
    "\n",
    "print(f\"CSV exported to: {csv_path}\")\n",
    "print(f\"JSON exported to: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ce627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export instances DataFrame as parquet\n",
    "if instances_df is not None and not instances_df.empty:\n",
    "    # Reset categorical columns to strings for better parquet compatibility\n",
    "    instances_export = instances_df.copy()\n",
    "    for col in instances_export.select_dtypes(include=['category']).columns:\n",
    "        instances_export[col] = instances_export[col].astype(str)\n",
    "    \n",
    "    parquet_path = os.path.join(exports_path, f\"{dataset_name}_instances.parquet\")\n",
    "    instances_export.to_parquet(parquet_path, index=False)\n",
    "    print(f\"Exported instances DataFrame to: {parquet_path}\")\n",
    "    print(f\"Shape: {instances_export.shape}\")\n",
    "    print(f\"File size: {os.path.getsize(parquet_path) / (1024*1024):.2f} MB\")\n",
    "    print(f\"Columns: {list(instances_export.columns)}\")\n",
    "    print(f\"Linked to frequencies via 'shape_id' column\")\n",
    "else:\n",
    "    print(\"No instances DataFrame available to export\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
