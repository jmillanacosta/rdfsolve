{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d16b10",
   "metadata": {},
   "source": [
    "# pubchem.source Schema Extraction\n",
    "\n",
    "This notebook extracts RDF schema from the pubchem.source SPARQL endpoint. It attempts discovery of existing VoID (Vocabulary of Interlinked Datasets) metadata first, then generates VoID from queries if needed. All downstream outputs are generated from the VoID description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5492f",
   "metadata": {},
   "source": [
    "## Exports\n",
    "\n",
    "- [JSON-LD Schema](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/data/schema_extraction/pubchem.source/pubchem.source_schema.jsonld) (primary output)\n",
    "- [N-Quads RDF](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/data/schema_extraction/pubchem.source/pubchem.source_schema.nq)\n",
    "- [VoID Graph](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/data/schema_extraction/pubchem.source/pubchem.source_generated_void.ttl)\n",
    "- [Coverage Report](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/data/schema_extraction/pubchem.source/pubchem.source_pattern_coverage.csv)\n",
    "- [LinkML Schema](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/data/schema_extraction/pubchem.source/pubchem.source_linkml_schema.yaml)\n",
    "- [Instance Data](https://github.com/jmillanacosta/rdfsolve/blob/main/docs/data/schema_extraction/pubchem.source/pubchem.source_instances_subject_index.json) (subject index, object index, and instances in JSON Lines format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ded37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Configuration\n",
    "import os\n",
    "\n",
    "# Dataset parameters\n",
    "endpoint_url = \"https://idsm.elixir-czech.cz/sparql/endpoint/idsm\"\n",
    "dataset_name = \"pubchem.source\"\n",
    "void_iri = \"http://rdf.ncbi.nlm.nih.gov/pubchem/source\"\n",
    "graph_uri = \"http://rdf.ncbi.nlm.nih.gov/pubchem/source\"\n",
    "\n",
    "# Setup paths\n",
    "working_path = os.path.abspath(\"\")\n",
    "exports_path = os.path.join(\n",
    "    working_path, \"..\", \"..\", \"docs\", \"data\", \"schema_extraction\", dataset_name\n",
    ")\n",
    "os.makedirs(exports_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Minimal notebook logger using existing dataset_name\n",
    "logger = logging.getLogger(dataset_name or \"notebook\")\n",
    "logger.setLevel(logging.DEBUG)  # Set to DEBUG to see SPARQL queries\n",
    "\n",
    "# Also configure the rdfsolve.parser logger to see query details\n",
    "parser_logger = logging.getLogger(\"rdfsolve.parser\")\n",
    "parser_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Avoid adding duplicate handlers if the cell is re-run\n",
    "if not logger.handlers:\n",
    "    fmt = logging.Formatter(\"%(asctime)s %(levelname)s %(name)s: %(message)s\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    sh = logging.StreamHandler(sys.stdout)\n",
    "    sh.setLevel(logging.DEBUG)  # Set to DEBUG to see all logs\n",
    "    sh.setFormatter(fmt)\n",
    "    logger.addHandler(sh)\n",
    "\n",
    "    # Add the same handler to the parser logger\n",
    "    parser_logger.addHandler(sh)\n",
    "\n",
    "logger.info(f\"Logging configured for {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cfa4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "\n",
    "# Configure Plotly for HTML output\n",
    "import plotly.io as pio\n",
    "import plotly.offline as pyo\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Import rdfsolve API functions\n",
    "from rdfsolve.api import (\n",
    "    discover_void_graphs,\n",
    "    generate_void_from_endpoint,\n",
    "    load_parser_from_graph,\n",
    "    retrieve_void_from_graphs,\n",
    ")\n",
    "from rdfsolve.sparql_helper import SparqlHelper\n",
    "\n",
    "# Enable query collection to track all SPARQL queries executed\n",
    "SparqlHelper.enable_query_collection()\n",
    "\n",
    "# Set renderer to 'notebook' for Jupyter, but ensure HTML export works\n",
    "pio.renderers.default = \"notebook+plotly_mimetype\"\n",
    "\n",
    "# Initialize offline mode for Plotly\n",
    "pyo.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d243f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle caching utilities\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def save_cache(data, filename, cache_dir=None):\n",
    "    \"\"\"Save data to pickle cache.\"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(exports_path, \"cache\")\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    cache_path = os.path.join(cache_dir, f\"{filename}.pkl\")\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Cached data to: {cache_path}\")\n",
    "    return cache_path\n",
    "\n",
    "\n",
    "def load_cache(filename, cache_dir=None):\n",
    "    \"\"\"Load data from pickle cache if it exists.\"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(exports_path, \"cache\")\n",
    "\n",
    "    cache_path = os.path.join(cache_dir, f\"{filename}.pkl\")\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"Loaded cached data from: {cache_path}\")\n",
    "        return data\n",
    "    return None\n",
    "\n",
    "\n",
    "def cache_exists(filename, cache_dir=None):\n",
    "    \"\"\"Check if cache file exists.\"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(exports_path, \"cache\")\n",
    "\n",
    "    cache_path = os.path.join(cache_dir, f\"{filename}.pkl\")\n",
    "    return os.path.exists(cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache management utilities\n",
    "def list_cache_files(cache_dir=None):\n",
    "    \"\"\"List all cache files.\"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(exports_path, \"cache\")\n",
    "\n",
    "    if not os.path.exists(cache_dir):\n",
    "        print(\"No cache directory found\")\n",
    "        return []\n",
    "\n",
    "    cache_files = [f for f in os.listdir(cache_dir) if f.endswith(\".pkl\")]\n",
    "    print(f\"Cache directory: {cache_dir}\")\n",
    "    for f in cache_files:\n",
    "        file_path = os.path.join(cache_dir, f)\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"  {f} ({size_mb:.2f} MB)\")\n",
    "    return cache_files\n",
    "\n",
    "\n",
    "def clear_cache(filename=None, cache_dir=None):\n",
    "    \"\"\"Clear specific cache file or all cache.\"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(exports_path, \"cache\")\n",
    "\n",
    "    if filename:\n",
    "        cache_path = os.path.join(cache_dir, f\"{filename}.pkl\")\n",
    "        if os.path.exists(cache_path):\n",
    "            os.remove(cache_path)\n",
    "            print(f\"Removed cache: {filename}\")\n",
    "        else:\n",
    "            print(f\"Cache not found: {filename}\")\n",
    "    else:\n",
    "        # Clear all cache files\n",
    "        if os.path.exists(cache_dir):\n",
    "            import shutil\n",
    "\n",
    "            shutil.rmtree(cache_dir)\n",
    "            print(\"Cleared all cache files\")\n",
    "        else:\n",
    "            print(\"No cache directory to clear\")\n",
    "\n",
    "\n",
    "# Show current cache status\n",
    "list_cache_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd27dea",
   "metadata": {},
   "source": [
    "### Cache Control\n",
    "\n",
    "Use these cells to manage cached data. When testing new code changes, you may want to clear relevant cache files to force re-computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear specific cache files (uncomment lines as needed for testing)\n",
    "\n",
    "# When testing new VoID discovery/generation:\n",
    "# clear_cache(f\"{dataset_name}_voidgraph\")\n",
    "\n",
    "# When testing JSON-LD generation (primary output):\n",
    "# clear_cache(f\"{dataset_name}_jsonld_schema\")\n",
    "\n",
    "# When testing frequency calculations:\n",
    "# clear_cache(f\"{dataset_name}_frequencies_basic\")\n",
    "# clear_cache(f\"{dataset_name}_frequencies_with_instances\")\n",
    "\n",
    "# Clear everything:\n",
    "clear_cache()\n",
    "\n",
    "print(\"Cache control ready\")\n",
    "print(\"Note: VoID graph and JSON-LD are the primary caches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce8103",
   "metadata": {},
   "source": [
    "## VoID Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_key = f\"{dataset_name}_voidgraph\"\n",
    "void_graph = load_cache(cache_key)\n",
    "\n",
    "if void_graph is None:\n",
    "    discovery_result = discover_void_graphs(\n",
    "        endpoint_url, graph_uris=[graph_uri] if graph_uri else None\n",
    "    )\n",
    "\n",
    "    found_graphs = discovery_result.get(\"found_graphs\", [])\n",
    "    partitions = discovery_result.get(\"partitions\", [])\n",
    "\n",
    "    if found_graphs and partitions:\n",
    "        print(f\"Found {len(found_graphs)} VoID graphs with {len(partitions)} partitions\")\n",
    "        void_graph = retrieve_void_from_graphs(\n",
    "            endpoint_url,\n",
    "            found_graphs,\n",
    "            graph_uris=[graph_uri] if graph_uri else None,\n",
    "            partitions=partitions,\n",
    "        )\n",
    "        void_path = os.path.join(exports_path, f\"{dataset_name}_discovered_void.ttl\")\n",
    "        void_graph.serialize(destination=void_path, format=\"turtle\")\n",
    "        print(f\"Saved discovered VoID to: {void_path}\")\n",
    "    else:\n",
    "        print(\"No VoID found, generating from endpoint...\")\n",
    "        void_graph = generate_void_from_endpoint(\n",
    "            endpoint_url=endpoint_url,\n",
    "            graph_uris=[graph_uri] if graph_uri else None,\n",
    "            output_file=os.path.join(exports_path, f\"{dataset_name}_generated_void.ttl\"),\n",
    "            counts=True,\n",
    "            offset_limit_steps=300,\n",
    "            exclude_graphs=True,\n",
    "        )\n",
    "\n",
    "    save_cache(void_graph, cache_key)\n",
    "    print(f\"Cached VoID graph ({len(void_graph)} triples)\")\n",
    "else:\n",
    "    print(f\"Loaded from cache ({len(void_graph)} triples)\")\n",
    "\n",
    "vp = load_parser_from_graph(void_graph, graph_uris=[graph_uri] if graph_uri else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e848a08",
   "metadata": {},
   "source": [
    "## Schema Discovery and Exports Workflow\n",
    "\n",
    "### Workflow Steps:\n",
    "\n",
    "1. **VoID Discovery**: Extract schema patterns from SPARQL endpoint VoID descriptions\n",
    "2. **JSON-LD Generation**: Convert to JSON-LD.\n",
    "3. **Derived Outputs**: All other formats are generated from the JSON-LD structure:\n",
    "   - **Frequencies**: Schema pattern coverage analysis\n",
    "   - **LinkML**: LinkML YAML used elsewhere for other features.\n",
    "   - **CSV/JSON**: Tabular and structured data exports\n",
    "   - **RDF**: N-Quads serialization for triplestore import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b09ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary JSON-LD schema export and basic summary\n",
    "cache_key = f\"{dataset_name}_jsonld_schema\"\n",
    "jsonld_schema = load_cache(cache_key)\n",
    "\n",
    "if jsonld_schema is None:\n",
    "    print(\"Generating JSON-LD schema...\")\n",
    "    jsonld_schema = vp.to_jsonld(filter_void_admin_nodes=True)\n",
    "    save_cache(jsonld_schema, cache_key)\n",
    "else:\n",
    "    print(\"Loaded JSON-LD schema from cache\")\n",
    "\n",
    "# Save JSON-LD schema file\n",
    "jsonld_file = os.path.join(exports_path, f\"{dataset_name}_schema.jsonld\")\n",
    "with open(jsonld_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(jsonld_schema, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"JSON-LD Schema saved to: {jsonld_file}\")\n",
    "\n",
    "# Display combined JSON-LD structure info and schema summary\n",
    "if \"@graph\" in jsonld_schema:\n",
    "    print(\"\\nSchema Summary:\")\n",
    "    print(f\"   • Prefixes: {len(jsonld_schema['@context'])}\")\n",
    "    print(f\"   • Resources: {len(jsonld_schema['@graph'])}\")\n",
    "\n",
    "    # Show dataset metadata\n",
    "    dataset_info = jsonld_schema[\"@graph\"][0] if jsonld_schema[\"@graph\"] else {}\n",
    "    if dataset_info.get(\"@type\") == \"void:Dataset\":\n",
    "        print(f\"   • Dataset: {dataset_info.get('dcterms:title', 'Unknown')}\")\n",
    "        print(f\"   • Classes: {dataset_info.get('void:classes', 0)}\")\n",
    "        print(f\"   • Properties: {dataset_info.get('void:properties', 0)}\")\n",
    "        print(f\"   • Triples: {dataset_info.get('void:triples', 0)}\")\n",
    "\n",
    "# Get schema DataFrame and show sample\n",
    "schema_df = vp.to_schema(filter_void_admin_nodes=True)\n",
    "print(f\"\\nSchema Patterns Preview ({len(schema_df)} total):\")\n",
    "display(schema_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a530dd",
   "metadata": {},
   "source": [
    "## Schema Pattern Coverage Analysis\n",
    "Calculate coverage ratios showing what percentage of entities use each relationship pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b864b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema pattern coverage analysis and export\n",
    "cache_key = f\"{dataset_name}_frequencies_basic\"\n",
    "cached_data = load_cache(cache_key)\n",
    "\n",
    "if cached_data is None:\n",
    "    print(\"Calculating schema pattern frequencies...\")\n",
    "    frequencies_df, _ = vp.count_schema_shape_frequencies(\n",
    "        endpoint_url=endpoint_url,\n",
    "        offset_limit_steps=300,\n",
    "    )\n",
    "    save_cache(frequencies_df, cache_key)\n",
    "else:\n",
    "    print(\"Loaded frequencies DataFrame from cache\")\n",
    "    frequencies_df = cached_data\n",
    "\n",
    "# Export coverage analysis\n",
    "frequencies_output_path = os.path.join(exports_path, f\"{dataset_name}_pattern_coverage.csv\")\n",
    "exported_df = vp.export_schema_shape_frequencies(\n",
    "    frequencies_df, output_file=frequencies_output_path\n",
    ")\n",
    "\n",
    "# Combined summary and sample\n",
    "if not frequencies_df.empty:\n",
    "    avg_coverage = frequencies_df[\"coverage_percent\"].mean()\n",
    "    high_coverage = (frequencies_df[\"coverage_percent\"] > 50).sum()\n",
    "\n",
    "    print(\"\\nPattern Coverage Analysis:\")\n",
    "    print(f\"   • Total patterns: {len(frequencies_df)}\")\n",
    "    print(f\"   • Average coverage: {avg_coverage:.1f}%\")\n",
    "    print(f\"   • High coverage (>50%): {high_coverage}\")\n",
    "    print(f\"   • Exported to: {frequencies_output_path}\")\n",
    "\n",
    "    print(\"\\nSample Coverage Data:\")\n",
    "    display(\n",
    "        frequencies_df[[\"subject_class\", \"property\", \"object_class\", \"coverage_percent\"]].head()\n",
    "    )\n",
    "\n",
    "    print(\"\\nCoverage Statistics:\")\n",
    "    display(frequencies_df[\"coverage_percent\"].describe())\n",
    "else:\n",
    "    print(\"No frequency data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a15c1",
   "metadata": {},
   "source": [
    "## Schema Pattern Instance Collection\n",
    "Collect actual subject and object IRI instances for each schema pattern. This provides detailed access to the specific entities participating in each relationship pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5833f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect both frequency data and actual instances with caching\n",
    "cache_key = f\"{dataset_name}_frequencies_with_instances\"\n",
    "cached_data = load_cache(cache_key)\n",
    "\n",
    "if cached_data is None:\n",
    "    print(\"Collecting frequency data and instances...\")\n",
    "    frequencies_with_instances_df, instances_df = vp.count_schema_shape_frequencies(\n",
    "        endpoint_url=endpoint_url,\n",
    "        # sample_limit=100,  # Limited sample for demonstration\n",
    "        collect_instances=True,\n",
    "        offset_limit_steps=300,\n",
    "    )\n",
    "    # Cache both DataFrames as a tuple\n",
    "    save_cache((frequencies_with_instances_df, instances_df), cache_key)\n",
    "else:\n",
    "    print(\"Loaded frequencies and instances DataFrames from cache\")\n",
    "    frequencies_with_instances_df, instances_df = cached_data\n",
    "\n",
    "# Display basic information about the data structure\n",
    "print(f\"Frequencies DataFrame: {len(frequencies_with_instances_df)} shapes\")\n",
    "if instances_df is not None:\n",
    "    print(f\"Instances DataFrame: {len(instances_df)} relationships\")\n",
    "    print(f\"Memory usage: {instances_df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "    # Export instances in compact format\n",
    "    instances_prefix = os.path.join(exports_path, f\"{dataset_name}_instances\")\n",
    "    export_result = vp.export_instances_compact(instances_df, instances_prefix)\n",
    "    print(f\"Exported instances: {export_result['total_relationships']} relationships\")\n",
    "    print(f\"   • Subject index: {export_result['total_subjects']} unique subjects\")\n",
    "    print(f\"   • Object index: {export_result['total_objects']} unique objects\")\n",
    "    print(\n",
    "        f\"   • Total size: {sum([export_result['subject_index_size_mb'], export_result['object_index_size_mb'], export_result['instances_jsonl_size_mb']]):.2f} MB\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No instances collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f98d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export instances in compact format\n",
    "if instances_df is not None:\n",
    "    instances_prefix = os.path.join(exports_path, f\"{dataset_name}_instances\")\n",
    "    export_result = vp.export_instances_compact(instances_df, instances_prefix)\n",
    "    print(f\"\\nExported instances: {export_result['total_relationships']} relationships\")\n",
    "    print(f\"   • Subject index: {export_result['total_subjects']} unique subjects\")\n",
    "    print(f\"   • Object index: {export_result['total_objects']} unique objects\")\n",
    "    print(\n",
    "        f\"   • Total size: {sum([export_result['subject_index_size_mb'], export_result['object_index_size_mb'], export_result['instances_jsonl_size_mb']]):.2f} MB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db811da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "if not frequencies_with_instances_df.empty:\n",
    "    df = frequencies_with_instances_df.copy()\n",
    "    df[\"coverage_percent\"] = pd.to_numeric(df[\"coverage_percent\"], errors=\"coerce\").fillna(0)\n",
    "    df = df.sort_values(\"coverage_percent\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    def make_label(row):\n",
    "        return (\n",
    "            f\"<b>{row['subject_class']}</b> \"\n",
    "            f\"<span style='color:#888;'></span> \"\n",
    "            f\"<i>{row['property']}</i> \"\n",
    "            f\"<span style='color:#888;'></span> \"\n",
    "            f\"<b>{row['object_class']}</b>\"\n",
    "        )\n",
    "\n",
    "    df[\"styled_label\"] = df.apply(make_label, axis=1)\n",
    "\n",
    "    text_positions = [\"outside\" if v < 95 else \"inside\" for v in df[\"coverage_percent\"]]\n",
    "    custom_colorscale = [\n",
    "        [0.0, \"#d36e61\"],\n",
    "        [0.4, \"#e5cdbd\"],\n",
    "        [0.7, \"#e8e4cf\"],\n",
    "        [1.0, \"#c3d9c0\"],\n",
    "    ]\n",
    "\n",
    "    # Figure sizing\n",
    "    bar_height = 26\n",
    "    fig_height = min(2000, bar_height * len(df) + 200)\n",
    "\n",
    "    fig = go.Figure(\n",
    "        go.Bar(\n",
    "            x=df[\"coverage_percent\"],\n",
    "            y=df[\"styled_label\"],\n",
    "            orientation=\"h\",\n",
    "            text=[f\"{v:.1f}%\" for v in df[\"coverage_percent\"]],\n",
    "            textposition=text_positions,\n",
    "            marker={\n",
    "                \"color\": df[\"coverage_percent\"],\n",
    "                \"colorscale\": custom_colorscale,\n",
    "                \"cmin\": 0,\n",
    "                \"cmax\": 100,\n",
    "                \"line\": {\"color\": \"white\", \"width\": 0.6},\n",
    "            },\n",
    "            hovertemplate=\"<b>%{y}</b><br>Coverage: %{x:.1f}%<extra></extra>\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            \"text\": f\"Schema Pattern Coverage for {dataset_name}\",\n",
    "            \"x\": 0.5,\n",
    "            \"font\": {\"size\": 18},\n",
    "        },\n",
    "        xaxis={\n",
    "            \"title\": \"Coverage (%)\",\n",
    "            \"range\": [0, 100],  # fixed x-axis range\n",
    "            \"ticksuffix\": \"%\",\n",
    "            \"showgrid\": True,\n",
    "            \"gridcolor\": \"rgba(220,220,220,0.3)\",\n",
    "        },\n",
    "        yaxis={\n",
    "            \"title\": \"\",\n",
    "            \"autorange\": \"reversed\",\n",
    "            \"automargin\": True,\n",
    "            \"fixedrange\": False,  # allow vertical zoom/pan\n",
    "        },\n",
    "        template=\"plotly_white\",\n",
    "        autosize=True,  # allow figure to scale with container\n",
    "        height=fig_height,  # base height (will scale)\n",
    "        margin={\"t\": 80, \"b\": 50, \"l\": 480, \"r\": 150},  # extra right margin for text\n",
    "        plot_bgcolor=\"white\",\n",
    "        paper_bgcolor=\"white\",\n",
    "    )\n",
    "\n",
    "    # Disable horizontal zoom/pan\n",
    "    fig.update_xaxes(fixedrange=True)\n",
    "\n",
    "    # Show figure with config for HTML export compatibility\n",
    "    fig.show(\n",
    "        config={\n",
    "            \"scrollZoom\": True,\n",
    "            \"responsive\": True,\n",
    "            \"toImageButtonOptions\": {\n",
    "                \"format\": \"png\",\n",
    "                \"filename\": f\"{dataset_name}_schema_coverage\",\n",
    "                \"height\": fig_height,\n",
    "                \"width\": 600,\n",
    "                \"scale\": 1,\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "else:\n",
    "    display(Markdown(\"**No coverage data to visualize**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9920263",
   "metadata": {},
   "source": [
    "## LinkML (derived from JSON-LD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2baa043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LinkML directly from JSON-LD with custom schema URI\n",
    "print(\"Regenerating LinkML schema from JSON-LD with custom schema URI...\")\n",
    "\n",
    "schema_name = f\"{dataset_name}_schema\"\n",
    "custom_schema_uri = (\n",
    "    f\"http://jmillanacosta.github.io/rdfsolve/{dataset_name}/linkml\"  # User-definable base URI\n",
    ")\n",
    "\n",
    "yaml_text = vp.to_linkml_yaml(\n",
    "    schema_name=schema_name,\n",
    "    schema_description=f\"LinkML schema for {dataset_name} generated from JSON-LD\",\n",
    "    schema_base_uri=custom_schema_uri,\n",
    "    filter_void_nodes=True,\n",
    ")\n",
    "\n",
    "# Save to LinkML YAML\n",
    "linkml_file = os.path.join(exports_path, f\"{dataset_name}_linkml_schema.yaml\")\n",
    "with open(linkml_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(yaml_text)\n",
    "\n",
    "print(f\"LinkML YAML saved to: {linkml_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkml.generators.erdiagramgen import ERDiagramGenerator\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "\n",
    "sv = SchemaView(linkml_file)\n",
    "linkml_schema = sv.schema\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"**Parsed LinkML schema:** Classes = {len(sv.all_classes())}, Slots = {len(sv.all_slots())}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build and display a Mermaid class diagram for the aopwikirdf LinkedML\n",
    "mermaid_code = ERDiagramGenerator(linkml_file).serialize()\n",
    "\n",
    "display(Markdown(mermaid_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = os.path.join(exports_path, f\"{dataset_name}_schema.json\")\n",
    "csv_path = os.path.join(exports_path, f\"{dataset_name}_schema.csv\")\n",
    "\n",
    "# Export CSV from frequencies\n",
    "frequencies_df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Export JSON derived from JSON-LD (maintains consistency)\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(vp.to_json(filter_void_nodes=True), fh, indent=2)\n",
    "\n",
    "print(f\"CSV exported to: {csv_path}\")\n",
    "print(f\"JSON exported to: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb4e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export collected SPARQL queries as TTL\n",
    "queries_path = os.path.join(exports_path, f\"{dataset_name}_sparql_queries.ttl\")\n",
    "queries = SparqlHelper.get_collected_queries()\n",
    "\n",
    "if queries:\n",
    "    ttl_content = SparqlHelper.export_queries_as_ttl(\n",
    "        output_file=queries_path,\n",
    "        base_uri=f\"https://github.com/jmillanacosta/rdfsolve/sparql/{dataset_name}/\",\n",
    "        dataset_name=dataset_name,\n",
    "    )\n",
    "    print(f\"Exported {len(queries)} SPARQL queries to: {queries_path}\")\n",
    "else:\n",
    "    print(\"No SPARQL queries were collected\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
