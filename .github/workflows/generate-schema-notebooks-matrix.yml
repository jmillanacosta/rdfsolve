name: Generate Schema Notebooks (Matrix)

on:
  workflow_dispatch:
    inputs:
      max_parallel:
        description: 'Maximum number of datasets to process in parallel (default: no limit)'
        required: false
        type: string
  schedule:
    # Run weekly on Sundays at 02:00 UTC
    - cron: '0 2 * * 0'

# Prevent concurrent runs of this workflow
concurrency:
  group: schema-notebooks-matrix
  cancel-in-progress: false

permissions:
  contents: write
  actions: read

jobs:
  prepare-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
    - uses: actions/checkout@v4
    
    - name: Generate dataset matrix
      id: set-matrix
      run: |
        cd notebooks/schema_extraction
        
        # Extract dataset names from sources.csv and create proper JSON
        datasets=$(tail -n +2 ../../data/sources.csv | cut -d',' -f1 | grep -v '^$' | jq -R . | jq -s . | jq -c .)
        
        # Create matrix for parallel processing
        echo "datasets=$datasets" >> $GITHUB_OUTPUT
        echo "matrix={\"dataset\":$datasets}" >> $GITHUB_OUTPUT
        
        # Log for debugging
        dataset_count=$(echo $datasets | jq length)
        echo "Generated matrix with $dataset_count datasets"
        echo "All datasets will be processed in parallel"

  generate-notebooks:
    needs: prepare-matrix
    runs-on: ubuntu-latest
    timeout-minutes: 45
    continue-on-error: true
    strategy:
      matrix: ${{fromJson(needs.prepare-matrix.outputs.matrix)}}
      max-parallel: ${{ github.event.inputs.max_parallel || 50 }}
      fail-fast: false
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y pandoc

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[notebooks]
        pip install jupyter nbconvert pandas

    - name: Generate notebook for ${{ matrix.dataset }}
      run: |
        cd notebooks/schema_extraction
        echo "Generating notebook for dataset: ${{ matrix.dataset }}"
        python make_notebooks.py --dataset "${{ matrix.dataset }}"

    - name: Execute notebook and convert to HTML
      id: convert
      run: |
        cd notebooks/schema_extraction
        notebook="${{ matrix.dataset }}_schema.ipynb"
        
        # Create HTML output directory
        mkdir -p ../../docs/notebooks/schema_extraction
        
        echo "Processing: $notebook"
        
        # Set timeout and memory limits
        #export JUPYTER_TIMEOUT=1800  # 30 minutes
        
        # Execute and convert with comprehensive error handling
        if jupyter nbconvert \
            --execute \
            --to html_embed \
            "$notebook" \
            --output-dir ../../docs/notebooks/schema_extraction \
            --ExecutePreprocessor.kernel_name=python3 \
            --no-input \
            --template=classic; then
          echo "SUCCESS=true" >> $GITHUB_OUTPUT
          echo "Successfully converted: $notebook"
        else
          echo "SUCCESS=false" >> $GITHUB_OUTPUT
          echo "Failed to convert: $notebook"
          
          # Create error report HTML
          base_name=$(basename "$notebook" .ipynb)
          cat > "../../docs/notebooks/schema_extraction/${base_name}.html" << 'EOF'
        <!DOCTYPE html>
        <html>
        <head>
            <title>Schema Analysis Failed - ${{ matrix.dataset }}</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 40px; }
                .error { color: #d32f2f; background: #ffebee; padding: 20px; border-radius: 4px; margin: 20px 0; }
                .info { background: #e3f2fd; padding: 15px; border-radius: 4px; margin: 20px 0; }
                .code { background: #f5f5f5; padding: 10px; border-radius: 4px; font-family: monospace; }
                h1 { color: #d32f2f; }
                h2 { color: #1976d2; }
            </style>
        </head>
        <body>
            <h1>Schema Analysis Failed</h1>
            
            <div class="info">
                <h2>Dataset Information</h2>
                <p><strong>Dataset:</strong> ${{ matrix.dataset }}</p>
                <p><strong>Attempted:</strong> $(date)</p>
                <p><strong>Workflow:</strong> Matrix-based parallel processing</p>
            </div>
            
            <div class="error">
                <h2>Possible Causes</h2>
                <ul>
                    <li><strong>Network timeout:</strong> SPARQL endpoint took too long to respond</li>
                    <li><strong>Endpoint unavailable:</strong> Service temporarily down or unreachable</li>
                    <li><strong>Dataset size:</strong> Too large for automated processing within time limits</li>
                    <li><strong>Query complexity:</strong> Schema extraction queries too complex for this dataset</li>
                    <li><strong>Rate limiting:</strong> Too many concurrent requests to the same endpoint</li>
                </ul>
            </div>
            
            <div class="info">
                <h2>Troubleshooting</h2>
                <p>This failure is expected for some datasets due to external dependencies. You can:</p>
                <ul>
                    <li>Run the notebook manually with custom timeouts</li>
                    <li>Use sampling parameters for large datasets</li>
                    <li>Check endpoint availability and try again later</li>
                    <li>Contact the endpoint maintainer if consistently failing</li>
                </ul>
            </div>
            
            <div class="info">
                <h2>Manual Execution</h2>
                <p>To run this analysis manually:</p>
                <div class="code">
                    cd notebooks/schema_extraction<br>
                    python make_notebooks.py --dataset ${{ matrix.dataset }}<br>
                    jupyter nbconvert --execute --to html ${{ matrix.dataset }}_schema.ipynb
                </div>
            </div>
            
            <p><em>Auto-generated error report by RDFSolve workflow</em></p>
        </body>
        </html>
        EOF
        fi

    - name: Upload individual result
      uses: actions/upload-artifact@v4
      with:
        name: schema-${{ matrix.dataset }}
        path: |
          notebooks/schema_extraction/${{ matrix.dataset }}_schema.ipynb
          docs/notebooks/schema_extraction/${{ matrix.dataset }}_schema.html
        retention-days: 7
        if-no-files-found: warn

    - name: Job summary
      run: |
        if [ "${{ steps.convert.outputs.SUCCESS }}" == "true" ]; then
          echo "**${{ matrix.dataset }}**: Schema analysis completed successfully" >> $GITHUB_STEP_SUMMARY
        else
          echo "**${{ matrix.dataset }}**: Schema analysis failed (timeout or error)" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Add timing information
        echo "**Duration**: Job completed at $(date)" >> $GITHUB_STEP_SUMMARY

  collect-results:
    needs: generate-notebooks
    runs-on: ubuntu-latest
    if: always()
    permissions:
      contents: write
      actions: read
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        path: artifacts/
        pattern: schema-*
        merge-multiple: true
      continue-on-error: true

    - name: Collect and organize results
      run: |
        # Create final output directories
        mkdir -p docs/notebooks/schema_extraction
        mkdir -p notebooks/schema_extraction/
        
        # Check if artifacts directory exists and has content
        if [ ! -d "artifacts" ] || [ -z "$(ls -A artifacts/ 2>/dev/null)" ]; then
          echo "No artifacts found or artifacts directory is empty"
          echo "This might happen if all matrix jobs failed"
          
          # Create a basic error report
          cat > docs/notebooks/schema_extraction/error_report.html << 'EOF'
        <!DOCTYPE html>
        <html>
        <head>
            <title>Matrix Workflow Error</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 40px; }
                .error { color: #d32f2f; background: #ffebee; padding: 20px; border-radius: 4px; }
            </style>
        </head>
        <body>
            <h1>Matrix Workflow Error</h1>
            <div class="error">
                <p><strong>No artifacts were collected from the matrix jobs.</strong></p>
                <p>This indicates that all parallel processing jobs may have failed.</p>
                <p>Please check the individual job logs for more details.</p>
            </div>
        </body>
        </html>
        EOF
        else
          # Safely collect notebooks and HTML files
          echo "Collecting artifacts from matrix jobs..."
          
          # Count available artifacts first
          notebook_artifacts=$(find artifacts/ -name "*_schema.ipynb" 2>/dev/null | wc -l)
          html_artifacts=$(find artifacts/ -name "*_schema.html" 2>/dev/null | wc -l)
          
          echo "Found artifacts: $notebook_artifacts notebooks, $html_artifacts HTML files"
          
          # Copy files with error handling
          if [ "$notebook_artifacts" -gt 0 ]; then
            find artifacts/ -name "*_schema.ipynb" -exec cp {} notebooks/schema_extraction/ \; 2>/dev/null || echo "Some notebook copies failed"
          fi
          
          if [ "$html_artifacts" -gt 0 ]; then
            find artifacts/ -name "*_schema.html" -exec cp {} docs/notebooks/schema_extraction/ \; 2>/dev/null || echo "Some HTML copies failed"
          fi
        fi
        
        # Final count of collected files
        final_notebooks=$(find notebooks/schema_extraction/ -name "*_schema.ipynb" 2>/dev/null | wc -l)
        final_html=$(find docs/notebooks/schema_extraction/ -name "*_schema.html" 2>/dev/null | wc -l)
        
        echo "Final collection results:"
        echo "  Notebooks: $final_notebooks"
        echo "  HTML files: $final_html"

    - name: Create comprehensive index
      run: |
        cd docs/notebooks/schema_extraction
        
        # Count results
        total_files=$(ls -1 *_schema.html 2>/dev/null | wc -l)
        success_count=$(grep -L "Schema Analysis Failed" *_schema.html 2>/dev/null | wc -l)
        failed_count=$((total_files - success_count))
        
        # Update schema-reports div in existing docs/index.md file
        cd ../../..
        current_date=$(date +%d/%m/%Y)
        
        # Create index.md if it doesn't exist with basic structure
        if [ ! -f "docs/index.md" ]; then
          echo "# RDFSolve" > docs/index.md
          echo "" >> docs/index.md
          echo "RDF schema extraction and analysis toolkit." >> docs/index.md
          echo "" >> docs/index.md
          echo "## Latest Analysis Results" >> docs/index.md
          echo "" >> docs/index.md
          echo '<div class="schema-reports">' >> docs/index.md
          echo "<!-- This section is automatically updated by the workflow -->" >> docs/index.md
          echo "</div>" >> docs/index.md
          echo "" >> docs/index.md
        fi
        
        # Create new content for the div
        new_content_file=$(mktemp)
        echo "Last updated: $current_date" > "$new_content_file"
        echo "" >> "$new_content_file"
        
        # Add HTML links to each HTML report
        cd docs/notebooks/schema_extraction
        for html_file in *_schema.html; do
          if [ -f "$html_file" ]; then
            dataset_name=$(basename "$html_file" .html | sed 's/_schema$//')
            echo "<div><a href=\"notebooks/schema_extraction/$html_file\">$dataset_name</a> - $current_date</div>" >> "$new_content_file"
          fi
        done
        
        echo "" >> "$new_content_file"
        echo "**Total datasets analyzed**: $total_files" >> "$new_content_file"
        echo "**Successful**: $success_count" >> "$new_content_file"  
        echo "**Failed**: $failed_count" >> "$new_content_file"
        echo "" >> "$new_content_file"
        echo "Generated on: $current_date" >> "$new_content_file"
        
        cd ../../..
        
        # Replace content between div tags using sed
        sed -i '/<div class="schema-reports">/,/<\/div>/{//!d;}' docs/index.md
        sed -i '/<div class="schema-reports">/r '"$new_content_file" docs/index.md
        
        # Clean up
        rm -f "$new_content_file"
        
        # Create comprehensive index.html
        cat > index.html << EOF
        <!DOCTYPE html>
        <html>
        <head>
            <title>RDFSolve Schema Analysis - Matrix Results</title>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <style>
                body { 
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; 
                    margin: 0; 
                    padding: 20px;
                    line-height: 1.6; 
                    background: #f8f9fa;
                }
                .container { max-width: 1200px; margin: 0 auto; }
                .header { 
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    padding: 30px; 
                    border-radius: 12px; 
                    margin-bottom: 30px; 
                    text-align: center;
                }
                .stats {
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                    gap: 20px;
                    margin-bottom: 30px;
                }
                .stat-card {
                    background: white;
                    padding: 20px;
                    border-radius: 8px;
                    text-align: center;
                    box-shadow: 0 2px 10px rgba(0,0,0,0.1);
                }
                .stat-number { font-size: 2em; font-weight: bold; }
                .success-number { color: #28a745; }
                .error-number { color: #dc3545; }
                .total-number { color: #007bff; }
                .dataset-grid { 
                    display: grid; 
                    grid-template-columns: repeat(auto-fill, minmax(320px, 1fr)); 
                    gap: 20px; 
                }
                .dataset-card { 
                    background: white;
                    border-radius: 8px; 
                    padding: 20px; 
                    box-shadow: 0 2px 10px rgba(0,0,0,0.1);
                    transition: transform 0.2s;
                }
                .dataset-card:hover { 
                    transform: translateY(-2px);
                    box-shadow: 0 4px 20px rgba(0,0,0,0.15);
                }
                .dataset-name { 
                    font-weight: 600; 
                    font-size: 1.1em;
                    margin-bottom: 12px;
                    color: #2c3e50;
                }
                .status-success { 
                    color: #28a745; 
                    background: #d4edda;
                    padding: 6px 12px;
                    border-radius: 20px;
                    font-size: 0.9em;
                    font-weight: 500;
                }
                .status-error { 
                    color: #721c24; 
                    background: #f8d7da;
                    padding: 6px 12px;
                    border-radius: 20px;
                    font-size: 0.9em;
                    font-weight: 500;
                }
                .view-link {
                    display: inline-block;
                    margin-top: 12px;
                    padding: 8px 16px;
                    background: #007bff;
                    color: white;
                    text-decoration: none;
                    border-radius: 6px;
                    font-size: 0.9em;
                    transition: background 0.2s;
                }
                .view-link:hover {
                    background: #0056b3;
                    color: white;
                }
                .footer { 
                    margin-top: 50px; 
                    padding: 30px;
                    background: white;
                    border-radius: 8px;
                    text-align: center; 
                    color: #6c757d; 
                }
                .section-title {
                    font-size: 1.5em;
                    font-weight: 600;
                    margin: 30px 0 20px 0;
                    color: #2c3e50;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>RDFSolve Schema Analysis</h1>
                    <p>Matrix-based parallel processing results</p>
                    <p><strong>Generated:</strong> $(date)</p>
                </div>
                
                <div class="stats">
                    <div class="stat-card">
                        <div class="stat-number total-number">$total_files</div>
                        <div>Total Datasets</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number success-number">$success_count</div>
                        <div>Successful</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number error-number">$failed_count</div>
                        <div>Failed</div>
                    </div>
                </div>
                
                <div class="section-title">Analysis Results</div>
                <div class="dataset-grid">
        EOF
        
        # Add each dataset to the index
        for html_file in *_schema.html; do
          if [ -f "$html_file" ]; then
            dataset_name=$(basename "$html_file" .html | sed 's/_schema$//')
            
            # Check success/failure and get file size
            if grep -q "Schema Analysis Failed" "$html_file"; then
              status_class="status-error"
              status_icon="[FAIL]"
              status_text="Analysis Failed"
            else
              status_class="status-success"
              status_icon="[OK]"
              status_text="Completed Successfully"
            fi
            
            file_size=$(ls -lh "$html_file" | awk '{print $5}')
            
            cat >> index.html << EOF
                    <div class="dataset-card">
                        <div class="dataset-name">$status_icon $dataset_name</div>
                        <div class="$status_class">$status_text</div>
                        <div style="margin-top: 8px; font-size: 0.9em; color: #6c757d;">
                            Size: $file_size
                        </div>
                        <a href="$html_file" class="view-link">View Analysis Report â†’</a>
                    </div>
        EOF
          fi
        done
        
        cat >> index.html << EOF
                </div>
                
                <div class="footer">
                    <h3>RDFSolve</h3>
                    <p>Automated RDF schema extraction and analysis toolkit</p>
                    <p><strong>Repository:</strong> <a href="https://github.com/jmillanacosta/rdfsolve">github.com/jmillanacosta/rdfsolve</a></p>
                    <p><em>Matrix processing enables parallel analysis of multiple datasets</em></p>
                </div>
            </div>
        </body>
        </html>
        EOF

    - name: Commit and push all results
      run: |
        # Configure git with proper identity
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action Matrix"
        
        # Add all generated files
        git add docs/index.md notebooks/schema_extraction/*.ipynb docs/notebooks/schema_extraction/ 2>/dev/null || true
        
        # Check if there are changes to commit
        if git diff --cached --quiet; then
          echo "No changes to commit"
          exit 0
        fi
        
        # Count results for commit message
        total_files=$(find docs/notebooks/schema_extraction/ -name "*_schema.html" 2>/dev/null | wc -l)
        
        # Create commit message with details
        commit_msg="Matrix workflow: Generate schema notebooks and HTML

        Parallel processing results:
        - Total datasets processed: $total_files
        - Generated on: $(date -u +%Y-%m-%d\ %H:%M:%S\ UTC)
        - Workflow run: $GITHUB_RUN_ID"
        
        # Commit and push
        git commit -m "$commit_msg"
        git push origin main

    - name: Upload final results
      uses: actions/upload-artifact@v4
      with:
        name: schema-analysis-matrix-complete
        path: docs/notebooks/schema_extraction/
        retention-days: 30

    - name: Final summary
      run: |
        echo "## Matrix Schema Analysis Complete" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        total=$(ls docs/notebooks/schema_extraction/*_schema.html 2>/dev/null | wc -l)
        success=$(grep -L "Schema Analysis Failed" docs/notebooks/schema_extraction/*_schema.html 2>/dev/null | wc -l)
        failed=$((total - success))
        
        echo "**Results Summary:**" >> $GITHUB_STEP_SUMMARY
        echo "- Total Datasets: $total" >> $GITHUB_STEP_SUMMARY
        echo "- Successful: $success" >> $GITHUB_STEP_SUMMARY  
        echo "- Failed: $failed" >> $GITHUB_STEP_SUMMARY
        echo "- Success Rate: $(echo "scale=1; $success * 100 / $total" | bc -l)%" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Artifacts:** Download 'schema-analysis-matrix-complete' for all HTML reports" >> $GITHUB_STEP_SUMMARY
