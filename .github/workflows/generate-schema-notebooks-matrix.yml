name: Generate Schema and Pydantic Notebooks (Matrix)

on:
  workflow_dispatch:
    inputs:
      max_parallel:
        description: 'Maximum number of datasets to process in parallel (default: no limit)'
        required: false
        type: string
      notebook_type:
        description: 'Type of notebooks to generate (schema, pydantic, or all)'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - schema
          - pydantic
  schedule:
    # Run weekly on Sundays at 02:00 UTC
    - cron: '0 2 * * 0'

# Prevent concurrent runs of this workflow
concurrency:
  group: schema-notebooks-matrix
  cancel-in-progress: false

permissions:
  contents: write
  actions: read

jobs:
  prepare-matrix:
    runs-on: ubuntu-latest
    outputs:
      schema-matrix: ${{ steps.set-matrix.outputs.schema-matrix }}
      pydantic-matrix: ${{ steps.set-matrix.outputs.pydantic-matrix }}
    steps:
    - uses: actions/checkout@v4
    
    - name: Generate dataset matrix
      id: set-matrix
      run: |
        # Extract dataset names from sources.csv and create proper JSON
        datasets=$(tail -n +2 data/sources.csv | cut -d',' -f1 | grep -v '^$' | jq -R . | jq -s . | jq -c .)
        
        # Create matrices for different notebook types
        echo "schema-matrix={\"dataset\":$datasets}" >> $GITHUB_OUTPUT
        echo "pydantic-matrix={\"dataset\":$datasets}" >> $GITHUB_OUTPUT
        
        # Log for debugging
        dataset_count=$(echo $datasets | jq length)
        echo "Generated matrix with $dataset_count datasets"
        echo "Notebook type requested: ${{ github.event.inputs.notebook_type || 'all' }}"

  generate-schema-notebooks:
    needs: prepare-matrix
    if: ${{ github.event.inputs.notebook_type == 'all' || github.event.inputs.notebook_type == 'schema' || github.event.inputs.notebook_type == '' }}
    runs-on: ubuntu-latest
    timeout-minutes: 45
    continue-on-error: true
    strategy:
      matrix: ${{fromJson(needs.prepare-matrix.outputs.schema-matrix)}}
      max-parallel: ${{ github.event.inputs.max_parallel || 50 }}
      fail-fast: false
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y pandoc

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[notebooks]
        pip install jupyter nbconvert pandas

    - name: Generate schema notebook for ${{ matrix.dataset }}
      run: |
        cd notebooks
        echo "Generating schema notebook for dataset: ${{ matrix.dataset }}"
        python make_notebooks.py --dataset "${{ matrix.dataset }}" --type schema

    - name: Execute schema notebook and convert to HTML
      id: convert
      run: |
        cd notebooks/01_schema_extraction
        notebook="${{ matrix.dataset }}_schema.ipynb"
        
        # Create HTML output directory
        mkdir -p ../../docs/notebooks/01_schema_extraction
        
        echo "Processing: $notebook"
        
        # Execute and convert to html
        if jupyter nbconvert \
            --execute \
            --to html \
            "$notebook" \
            --output-dir ../../docs/notebooks/01_schema_extraction \
            --ExecutePreprocessor.kernel_name=python3 ; then
          echo "SUCCESS=true" >> $GITHUB_OUTPUT
          echo "Successfully converted: $notebook"
        else
          echo "SUCCESS=false" >> $GITHUB_OUTPUT
          echo "Failed to convert: $notebook"
          
          # Create error report HTML
          base_name=$(basename "$notebook" .ipynb)
          cat > "../../docs/notebooks/01_schema_extraction/${base_name}.html" << 'EOF'
        <!DOCTYPE html>
        <html>
        <head>
            <title>Schema Analysis Failed - ${{ matrix.dataset }}</title>
        </head>
        <body>
            <h1>Schema Analysis Failed</h1>
            <h2>Dataset Information</h2>
            <p><strong>Dataset:</strong> ${{ matrix.dataset }}</p>
            <p><strong>Attempted:</strong> <script>document.write(new Date().toUTCString())</script></p>
            <p><strong>Workflow:</strong> <a href="https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}">Matrix-based parallel processing</a></p>
            <h2>Manual Execution</h2>
            <pre>cd notebooks/schema_extraction
            python make_notebooks.py --dataset ${{ matrix.dataset }}
            jupyter nbconvert --execute --to html ${{ matrix.dataset }}_schema.ipynb</pre>
          </body>
        </html>
        EOF
        fi

    - name: Upload individual result
      uses: actions/upload-artifact@v4
      with:
        name: schema-${{ matrix.dataset }}
        path: |
          notebooks/01_schema_extraction/${{ matrix.dataset }}_schema.ipynb
          docs/notebooks/01_schema_extraction/${{ matrix.dataset }}_schema.html
          data/**
        retention-days: 7
        if-no-files-found: warn

    - name: Job summary
      run: |
        if [ "${{ steps.convert.outputs.SUCCESS }}" == "true" ]; then
          echo "**${{ matrix.dataset }}**: Schema analysis completed successfully" >> $GITHUB_STEP_SUMMARY
        else
          echo "**${{ matrix.dataset }}**: Schema analysis failed (timeout or error)" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Add timing information
        echo "**Duration**: Job completed at $(date)" >> $GITHUB_STEP_SUMMARY

  generate-pydantic-notebooks:
    needs: [prepare-matrix, generate-schema-notebooks]
    if: ${{ !failure() && (github.event.inputs.notebook_type == 'all' || github.event.inputs.notebook_type == 'pydantic') }}
    runs-on: ubuntu-latest
    timeout-minutes: 30
    continue-on-error: true
    strategy:
      matrix: ${{fromJson(needs.prepare-matrix.outputs.pydantic-matrix)}}
      max-parallel: ${{ github.event.inputs.max_parallel || 50 }}
      fail-fast: false
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y pandoc

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[notebooks]
        pip install jupyter nbconvert pandas

    - name: Generate pydantic notebook for ${{ matrix.dataset }}
      run: |
        cd notebooks
        echo "Generating pydantic notebook for dataset: ${{ matrix.dataset }}"
        python make_notebooks.py --dataset "${{ matrix.dataset }}" --type pydantic

    - name: Execute pydantic notebook and convert to HTML
      id: convert-pydantic
      run: |
        cd notebooks/02_pydantic_models
        notebook="${{ matrix.dataset }}_pydantic.ipynb"
        
        # Create HTML output directory
        mkdir -p ../../docs/notebooks/02_pydantic_models
        
        echo "Processing: $notebook"
        
        # Execute and convert to html
        if jupyter nbconvert \
            --execute \
            --to html \
            "$notebook" \
            --output-dir ../../docs/notebooks/02_pydantic_models \
            --ExecutePreprocessor.kernel_name=python3 ; then
          echo "SUCCESS=true" >> $GITHUB_OUTPUT
          echo "Successfully converted: $notebook"
        else
          echo "SUCCESS=false" >> $GITHUB_OUTPUT
          echo "Failed to convert: $notebook"
          
          # Create error report HTML
          base_name=$(basename "$notebook" .ipynb)
          cat > "../../docs/notebooks/02_pydantic_models/${base_name}.html" << 'EOF'
        <!DOCTYPE html>
        <html>
        <head>
            <title>Pydantic Generation Failed - ${{ matrix.dataset }}</title>
        </head>
        <body>
            <h1>Pydantic Generation Failed</h1>
            <h2>Dataset Information</h2>
            <p><strong>Dataset:</strong> ${{ matrix.dataset }}</p>
            <p><strong>Attempted:</strong> <script>document.write(new Date().toUTCString())</script></p>
            <p><strong>Workflow:</strong> <a href="https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}">Matrix-based parallel processing</a></p>
            <h2>Manual Execution</h2>
            <pre>cd notebooks
            python make_notebooks.py --dataset ${{ matrix.dataset }} --type pydantic
            cd 02_pydantic_models
            jupyter nbconvert --execute --to html ${{ matrix.dataset }}_pydantic.ipynb</pre>
          </body>
        </html>
        EOF
        fi

    - name: Upload individual pydantic result
      uses: actions/upload-artifact@v4
      with:
        name: pydantic-${{ matrix.dataset }}
        path: |
          notebooks/02_pydantic_models/${{ matrix.dataset }}_pydantic.ipynb
          docs/notebooks/02_pydantic_models/${{ matrix.dataset }}_pydantic.html
          data/**
        retention-days: 7
        if-no-files-found: warn

    - name: Pydantic job summary
      run: |
        if [ "${{ steps.convert-pydantic.outputs.SUCCESS }}" == "true" ]; then
          echo "**${{ matrix.dataset }}**: Pydantic model generation completed successfully" >> $GITHUB_STEP_SUMMARY
        else
          echo "**${{ matrix.dataset }}**: Pydantic model generation failed (timeout or error)" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Add timing information
        echo "**Duration**: Job completed at $(date)" >> $GITHUB_STEP_SUMMARY

  collect-results:
    needs: [generate-schema-notebooks, generate-pydantic-notebooks]
    runs-on: ubuntu-latest
    if: always()
    permissions:
      contents: write
      actions: read
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Download all artifacts
      uses: actions/download-artifact@v4
      with:
        path: artifacts/
        pattern: "{schema,pydantic}-*"
        merge-multiple: true
      continue-on-error: true

    - name: Collect and organize results
      run: |
        # Create final output directories
        mkdir -p docs/notebooks/01_schema_extraction
        mkdir -p docs/notebooks/02_pydantic_models
        mkdir -p notebooks/01_schema_extraction/
        mkdir -p notebooks/02_pydantic_models/
        
        # Check if artifacts directory exists and has content
        if [ ! -d "artifacts" ] || [ -z "$(ls -A artifacts/ 2>/dev/null)" ]; then
          echo "No artifacts found or artifacts directory is empty"
          echo "This might happen if all matrix jobs failed"
          
          # Create a basic error report
          cat > docs/notebooks/schema_extraction/error_report.html << 'EOF'
        <!DOCTYPE html>
        <html>
        <head>
            <title>Matrix Workflow Error</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 40px; }
                .error { color: #d32f2f; background: #ffebee; padding: 20px; border-radius: 4px; }
            </style>
        </head>
        <body>
            <h1>Matrix Workflow Error</h1>
            <div class="error">
                <p><strong>No artifacts were collected from the matrix jobs.</strong></p>
                <p>This indicates that all parallel processing jobs may have failed.</p>
                <p>Please check the individual job logs for more details.</p>
            </div>
        </body>
        </html>
        EOF
        else
          # Safely collect notebooks and HTML files
          echo "Collecting artifacts from matrix jobs..."
          
          # Count available artifacts first
          schema_notebooks=$(find artifacts/ -name "*_schema.ipynb" 2>/dev/null | wc -l)
          schema_html=$(find artifacts/ -name "*_schema.html" 2>/dev/null | wc -l)
          pydantic_notebooks=$(find artifacts/ -name "*_pydantic.ipynb" 2>/dev/null | wc -l)
          pydantic_html=$(find artifacts/ -name "*_pydantic.html" 2>/dev/null | wc -l)
          
          echo "Found artifacts: $schema_notebooks schema notebooks, $schema_html schema HTML files"
          echo "Found artifacts: $pydantic_notebooks pydantic notebooks, $pydantic_html pydantic HTML files"
          
          # Copy files with error handling
          if [ "$schema_notebooks" -gt 0 ]; then
            find artifacts/ -name "*_schema.ipynb" -exec cp {} notebooks/01_schema_extraction/ \; 2>/dev/null || echo "Some schema notebook copies failed"
          fi
          
          if [ "$schema_html" -gt 0 ]; then
            find artifacts/ -name "*_schema.html" -exec cp {} docs/notebooks/01_schema_extraction/ \; 2>/dev/null || echo "Some schema HTML copies failed"
          fi
          
          if [ "$pydantic_notebooks" -gt 0 ]; then
            find artifacts/ -name "*_pydantic.ipynb" -exec cp {} notebooks/02_pydantic_models/ \; 2>/dev/null || echo "Some pydantic notebook copies failed"
          fi
          
          if [ "$pydantic_html" -gt 0 ]; then
            find artifacts/ -name "*_pydantic.html" -exec cp {} docs/notebooks/02_pydantic_models/ \; 2>/dev/null || echo "Some pydantic HTML copies failed"
          fi
        fi
        
        # Final count of collected files
        final_schema_notebooks=$(find notebooks/01_schema_extraction/ -name "*_schema.ipynb" 2>/dev/null | wc -l)
        final_schema_html=$(find docs/notebooks/01_schema_extraction/ -name "*_schema.html" 2>/dev/null | wc -l)
        final_pydantic_notebooks=$(find notebooks/02_pydantic_models/ -name "*_pydantic.ipynb" 2>/dev/null | wc -l)
        final_pydantic_html=$(find docs/notebooks/02_pydantic_models/ -name "*_pydantic.html" 2>/dev/null | wc -l)
        
        echo "Final collection results:"
        echo "  Schema notebooks: $final_schema_notebooks"
        echo "  Schema HTML files: $final_schema_html"
        echo "  Pydantic notebooks: $final_pydantic_notebooks"
        echo "  Pydantic HTML files: $final_pydantic_html"

    - name: Generate results data
      run: |
        # Count results from both directories
        total_schema_files=$(find docs/notebooks/01_schema_extraction/ -name "*_schema.html" 2>/dev/null | wc -l)
        total_pydantic_files=$(find docs/notebooks/02_pydantic_models/ -name "*_pydantic.html" 2>/dev/null | wc -l)
        
        schema_success_count=$(find docs/notebooks/01_schema_extraction/ -name "*_schema.html" -exec grep -L "Schema Analysis Failed" {} \; 2>/dev/null | wc -l)
        pydantic_success_count=$(find docs/notebooks/02_pydantic_models/ -name "*_pydantic.html" -exec grep -L "Pydantic Generation Failed" {} \; 2>/dev/null | wc -l)
        
        schema_failed_count=$((total_schema_files - schema_success_count))
        pydantic_failed_count=$((total_pydantic_files - pydantic_success_count))
        
        # Count generated data files
        data_files_count=$(find data/ -type f \( -name "*.jsonld" -o -name "*.yaml" -o -name "*.csv" -o -name "*.ttl" -o -name "*.nq" -o -name "*.parquet" \) 2>/dev/null | wc -l)
        
        # Generate JSON data for JavaScript consumption
        cat > docs/results.json << EOF
        {
          "lastUpdated": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "stats": {
            "totalDatasets": $total_schema_files,
            "successfulDatasets": $schema_success_count,
            "failedDatasets": $schema_failed_count,
            "totalDataFiles": $data_files_count
          },
          "schemaResults": [
        EOF
        
        # Process schema results
        first=true
        for html_file in docs/notebooks/01_schema_extraction/*_schema.html; do
          if [ -f "$html_file" ]; then
            dataset_name=$(basename "$html_file" .html | sed 's/_schema$//')
            
            # Check success/failure
            if grep -q "Schema Analysis Failed" "$html_file"; then
              status="error"
            else
              status="success"
            fi
            
            file_size=$(ls -lh "$html_file" | awk '{print $5}')
            report_url="notebooks/01_schema_extraction/$(basename $html_file)"
            
            # Add comma for all but first entry
            if [ "$first" = false ]; then
              echo "    ," >> docs/results.json
            fi
            first=false
            
            # Start dataset object
            cat >> docs/results.json << EOF
            {
              "name": "$dataset_name",
              "status": "$status",
              "reportSize": "$file_size",
              "reportUrl": "$report_url",
              "generated": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
              "dataFiles": {
        EOF
            
            # Check for data files
            data_first=true
            if [ -f "data/$dataset_name/${dataset_name}_schema.jsonld" ]; then
              if [ "$data_first" = false ]; then echo "        ," >> docs/results.json; fi
              echo "        \"jsonld\": \"../data/$dataset_name/${dataset_name}_schema.jsonld\"" >> docs/results.json
              data_first=false
            fi
            
            if [ -f "data/$dataset_name/${dataset_name}_linkml_schema.yaml" ]; then
              if [ "$data_first" = false ]; then echo "        ," >> docs/results.json; fi
              echo "        \"linkml\": \"../data/$dataset_name/${dataset_name}_linkml_schema.yaml\"" >> docs/results.json
              data_first=false
            fi
            
            if [ -f "data/$dataset_name/${dataset_name}_pattern_coverage.csv" ]; then
              if [ "$data_first" = false ]; then echo "        ," >> docs/results.json; fi
              echo "        \"coverage\": \"../data/$dataset_name/${dataset_name}_pattern_coverage.csv\"" >> docs/results.json
              data_first=false
            fi
            
            if [ -f "data/$dataset_name/${dataset_name}_generated_void.ttl" ]; then
              if [ "$data_first" = false ]; then echo "        ," >> docs/results.json; fi
              echo "        \"void\": \"../data/$dataset_name/${dataset_name}_generated_void.ttl\"" >> docs/results.json
              data_first=false
            fi
            
            if [ -f "data/$dataset_name/${dataset_name}_instances.parquet" ]; then
              if [ "$data_first" = false ]; then echo "        ," >> docs/results.json; fi
              echo "        \"instances\": \"../data/$dataset_name/${dataset_name}_instances.parquet\"" >> docs/results.json
              data_first=false
            fi
            
            if [ -f "data/$dataset_name/${dataset_name}_schema.nq" ]; then
              if [ "$data_first" = false ]; then echo "        ," >> docs/results.json; fi
              echo "        \"nquads\": \"../data/$dataset_name/${dataset_name}_schema.nq\"" >> docs/results.json
              data_first=false
            fi
            
            # Close dataset object
            cat >> docs/results.json << EOF
              }
            }
        EOF
          fi
        done
        
        # Close schema results and start pydantic results
        cat >> docs/results.json << EOF
          ],
          "pydanticResults": [
        EOF
        
        # Process pydantic results
        first=true
        for html_file in docs/notebooks/02_pydantic_models/*_pydantic.html; do
          if [ -f "$html_file" ]; then
            dataset_name=$(basename "$html_file" .html | sed 's/_pydantic$//')
            
            # Check success/failure
            if grep -q "Pydantic Generation Failed" "$html_file"; then
              status="error"
            else
              status="success"
            fi
            
            file_size=$(ls -lh "$html_file" | awk '{print $5}')
            report_url="notebooks/02_pydantic_models/$(basename $html_file)"
            
            # Add comma for all but first entry
            if [ "$first" = false ]; then
              echo "    ," >> docs/results.json
            fi
            first=false
            
            cat >> docs/results.json << EOF
            {
              "name": "$dataset_name",
              "status": "$status",
              "reportSize": "$file_size", 
              "reportUrl": "$report_url",
              "generated": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
            }
        EOF
          fi
        done
        
        # Close JSON structure
        cat >> docs/results.json << EOF
          ]
        }
        EOF
        
        echo "Generated results.json with $total_schema_files schema results and $total_pydantic_files pydantic results"


    - name: Commit and push all results
      run: |
        # Configure git with proper identity
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action Matrix"
        
        # Add all generated files including data outputs and web assets
        git add docs/results.json docs/index.html docs/style.css docs/app.js 2>/dev/null || true
        git add notebooks/01_schema_extraction/*.ipynb docs/notebooks/01_schema_extraction/ 2>/dev/null || true
        git add notebooks/02_pydantic_models/*.ipynb docs/notebooks/02_pydantic_models/ 2>/dev/null || true
        git add data/ 2>/dev/null || true
        
        # Check if there are changes to commit
        if git diff --cached --quiet; then
          echo "No changes to commit"
          exit 0
        fi
        
        # Count results for commit message
        schema_files=$(find docs/notebooks/01_schema_extraction/ -name "*_schema.html" 2>/dev/null | wc -l)
        pydantic_files=$(find docs/notebooks/02_pydantic_models/ -name "*_pydantic.html" 2>/dev/null | wc -l)
        data_files=$(find data/ -name "*.json*" -o -name "*.yaml" -o -name "*.csv" -o -name "*.ttl" -o -name "*.nq" -o -name "*.parquet" 2>/dev/null | wc -l)
        
        # Create commit message with details
        commit_msg="Matrix workflow: Generate schema notebooks, reports, and data files

        Parallel processing results:
        - Schema notebooks processed: $schema_files
        - Pydantic notebooks processed: $pydantic_files  
        - Data files generated: $data_files
        - Updated web dashboard with results.json
        - Generated on: $(date -u +%Y-%m-%d\ %H:%M:%S\ UTC)
        - Workflow run: $GITHUB_RUN_ID"
        
        # Commit and push
        git commit -m "$commit_msg"
        git push origin main

    - name: Upload final results
      uses: actions/upload-artifact@v4
      with:
        name: notebooks-matrix-complete
        path: |
          docs/notebooks/01_schema_extraction/
          docs/notebooks/02_pydantic_models/
        retention-days: 30

    - name: Final summary
      run: |
        echo "## Matrix Notebook Generation Complete" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        schema_total=$(find docs/notebooks/01_schema_extraction/ -name "*_schema.html" 2>/dev/null | wc -l)
        pydantic_total=$(find docs/notebooks/02_pydantic_models/ -name "*_pydantic.html" 2>/dev/null | wc -l)
        
        schema_success=$(find docs/notebooks/01_schema_extraction/ -name "*_schema.html" -exec grep -L "Schema Analysis Failed" {} \; 2>/dev/null | wc -l)
        pydantic_success=$(find docs/notebooks/02_pydantic_models/ -name "*_pydantic.html" -exec grep -L "Pydantic Generation Failed" {} \; 2>/dev/null | wc -l)
        
        schema_failed=$((schema_total - schema_success))
        pydantic_failed=$((pydantic_total - pydantic_success))
        
        echo "**Schema Analysis Results:**" >> $GITHUB_STEP_SUMMARY
        echo "- Total: $schema_total" >> $GITHUB_STEP_SUMMARY
        echo "- Successful: $schema_success" >> $GITHUB_STEP_SUMMARY  
        echo "- Failed: $schema_failed" >> $GITHUB_STEP_SUMMARY
        if [ "$schema_total" -gt 0 ]; then
          echo "- Success Rate: $(echo "scale=1; $schema_success * 100 / $schema_total" | bc -l)%" >> $GITHUB_STEP_SUMMARY
        fi
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Pydantic Model Results:**" >> $GITHUB_STEP_SUMMARY
        echo "- Total: $pydantic_total" >> $GITHUB_STEP_SUMMARY
        echo "- Successful: $pydantic_success" >> $GITHUB_STEP_SUMMARY  
        echo "- Failed: $pydantic_failed" >> $GITHUB_STEP_SUMMARY
        if [ "$pydantic_total" -gt 0 ]; then
          echo "- Success Rate: $(echo "scale=1; $pydantic_success * 100 / $pydantic_total" | bc -l)%" >> $GITHUB_STEP_SUMMARY
        fi
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Web Dashboard:** [View Results](https://jmillanacosta.github.io/rdfsolve/) (updated with results.json)" >> $GITHUB_STEP_SUMMARY
        echo "**Artifacts:** Download 'notebooks-matrix-complete' for all HTML reports" >> $GITHUB_STEP_SUMMARY
